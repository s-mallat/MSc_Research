# -*- coding: utf-8 -*-
"""SD_Walk.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1472qIDiretzZmSs_j_E_aGiMgft-nOCa

# Stable Diffusion - AnimationInPaint
Adapted from https://github.com/nateraw/stable-diffusion-videos AND https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion
"""

!pip install -qq -U diffusers==0.3.0 transformers ftfy
!pip install -qq "ipywidgets>=7,<8"

from huggingface_hub import notebook_login

notebook_login()

import json
import subprocess
from pathlib import Path
import PIL
import numpy as np
import torch
import inspect
from typing import List, Optional, Union
from diffusers.schedulers import (DDIMScheduler, LMSDiscreteScheduler,
                                  PNDMScheduler)
from diffusers import ModelMixin

#Import inpaint pipeline
from diffusers import StableDiffusionInpaintPipeline

has_cuda = torch.cuda.is_available()
device = torch.device('cpu' if not has_cuda else 'cuda')
print(device)

pipeline = StableDiffusionInpaintPipeline.from_pretrained(
    "CompVis/stable-diffusion-v1-4",
    use_auth_token=True,
    torch_dtype=torch.float16,
    revision="fp16",
).to("cuda")

default_scheduler = PNDMScheduler(
    beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear"
)
ddim_scheduler = DDIMScheduler(
    beta_start=0.00085,
    beta_end=0.012,
    beta_schedule="scaled_linear",
    clip_sample=False,
    set_alpha_to_one=False,
)
klms_scheduler = LMSDiscreteScheduler(
    beta_start=0.00085, beta_end=0.012, beta_schedule="scaled_linear"
)
SCHEDULERS = dict(default=default_scheduler, ddim=ddim_scheduler, klms=klms_scheduler)

def slerp(t, v0, v1, DOT_THRESHOLD=0.9995):
    """helper function to spherically interpolate two arrays v1 v2"""

    if not isinstance(v0, np.ndarray):
        inputs_are_torch = True
        input_device = v0.device
        v0 = v0.cpu().numpy()
        v1 = v1.cpu().numpy()

    dot = np.sum(v0 * v1 / (np.linalg.norm(v0) * np.linalg.norm(v1)))
    if np.abs(dot) > DOT_THRESHOLD:
        v2 = (1 - t) * v0 + t * v1
    else:
        theta_0 = np.arccos(dot)
        sin_theta_0 = np.sin(theta_0)
        theta_t = theta_0 * t
        sin_theta_t = np.sin(theta_t)
        s0 = np.sin(theta_0 - theta_t) / sin_theta_0
        s1 = sin_theta_t / sin_theta_0
        v2 = s0 * v0 + s1 * v1

    if inputs_are_torch:
        v2 = torch.from_numpy(v2).to(input_device)

    return v2

def make_video_ffmpeg(frame_dir, output_file_name='output.mp4', frame_filename="frame%06d.png", fps=30):
    frame_ref_path = str(frame_dir / frame_filename)
    video_path = str(frame_dir / output_file_name)
    subprocess.call(
        f"ffmpeg -r {fps} -i {frame_ref_path} -vcodec libx264 -crf 10 -pix_fmt yuv420p"
        f" {video_path}".split()
    )
    return video_path

import requests
from io import BytesIO

def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size
    
    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid


def download_image(url):
    response = requests.get(url)
    return PIL.Image.open(BytesIO(response.content)).convert("RGB")

img_url = "https://github.com/s-mallat/MSc_Research/blob/main/Research_Phase/FineTuning/init_image.png?raw=true"
mask_url = "https://github.com/s-mallat/MSc_Research/blob/main/Research_Phase/FineTuning/init2.png?raw=true"

init_image = download_image(img_url).resize((512, 512))
init_image

mask_image = download_image(mask_url).resize((512, 512))
mask_image

def walk(
    prompt=["blueberry spaghetti", "strawberry spaghetti"],
    seeds=[42, 123],
    num_steps=5,
    output_dir="dreams",
    name="berry_good_spaghetti",
    init_image=None,
    mask_image=None,
    height=512,
    width=512,
    guidance_scale=7.5,
    eta=0.0,
    num_inference_steps=50,
    do_loop=False,
    make_video=False,
    use_lerp_for_text=True,
    scheduler="klms",  # choices: default, ddim, klms
    disable_tqdm=False,
    upsample=False,
    fps=30,
    less_vram=False,
    resume=False,
    batch_size=1,
    frame_filename_ext='.png',
):
    """Generate video frames/a video given a list of prompts and seeds.

    Args:
        prompts (List[str], optional): List of . Defaults to ["blueberry spaghetti", "strawberry spaghetti"].
        seeds (List[int], optional): List of random seeds corresponding to given prompts.
        num_steps (int, optional): Number of steps to walk. Increase this value to 60-200 for good results. Defaults to 5.
        output_dir (str, optional): Root dir where images will be saved. Defaults to "dreams".
        name (str, optional): Sub directory of output_dir to save this run's files. Defaults to "berry_good_spaghetti".
        height (int, optional): Height of image to generate. Defaults to 512.
        width (int, optional): Width of image to generate. Defaults to 512.
        guidance_scale (float, optional): Higher = more adherance to prompt. Lower = let model take the wheel. Defaults to 7.5.
        eta (float, optional): ETA. Defaults to 0.0.
        num_inference_steps (int, optional): Number of diffusion steps. Defaults to 50.
        do_loop (bool, optional): Whether to loop from last prompt back to first. Defaults to False.
        make_video (bool, optional): Whether to make a video or just save the images. Defaults to False.
        use_lerp_for_text (bool, optional): Use LERP instead of SLERP for text embeddings when walking. Defaults to True.
        scheduler (str, optional): Which scheduler to use. Defaults to "klms". Choices are "default", "ddim", "klms".
        disable_tqdm (bool, optional): Whether to turn off the tqdm progress bars. Defaults to False.
        upsample (bool, optional): If True, uses Real-ESRGAN to upsample images 4x. Requires it to be installed
            which you can do by running: `pip install git+https://github.com/xinntao/Real-ESRGAN.git`. Defaults to False.
        fps (int, optional): The frames per second (fps) that you want the video to use. Does nothing if make_video is False. Defaults to 30.
        less_vram (bool, optional): Allow higher resolution output on smaller GPUs. Yields same result at the expense of 10% speed. Defaults to False.
        resume (bool, optional): When set to True, resume from provided '<output_dir>/<name>' path. Useful if your run was terminated
            part of the way through.
        batch_size (int, optional): Number of examples per batch fed to pipeline. Increase this until you
            run out of VRAM. Defaults to 1.
        frame_filename_ext (str, optional): File extension to use when saving/resuming. Update this to
            ".jpg" to save or resume generating jpg images instead. Defaults to ".png".

    Returns:
        str: Path to video file saved if make_video=True, else None.
    """
    if less_vram:
        pipeline.enable_attention_slicing()

    output_path = Path(output_dir) / name
    output_path.mkdir(exist_ok=True, parents=True)
    prompt_config_path = output_path / 'prompt_config.json'

    if not resume:
        # Write prompt info to file in output dir so we can keep track of what we did
        prompt_config_path.write_text(
            json.dumps(
                dict(
                    prompt=prompt,
                    seeds=seeds,
                    num_steps=num_steps,
                    name=name,
                    init_image=init_image,
                    mask_image=mask_image,
                    guidance_scale=guidance_scale,
                    eta=eta,
                    num_inference_steps=num_inference_steps,
                    do_loop=do_loop,
                    make_video=make_video,
                    use_lerp_for_text=use_lerp_for_text,
                    scheduler=scheduler,
                    upsample=upsample,
                    fps=fps,
                    height=height,
                    width=width,
                ),
                indent=2,
                sort_keys=False,
            )
        )
    else:
        # When resuming, we load all available info from existing prompt config, using kwargs passed in where necessary
        if not prompt_config_path.exists():
            raise FileNotFoundError(f"You specified resume=True, but no prompt config file was found at {prompt_config_path}")

        data = json.load(open(prompt_config_path))
        prompt = data['prompt']
        seeds = data['seeds']
        num_steps = data['num_steps']
        height = data['height'] if 'height' in data else height
        width = data['width'] if 'width' in data else width
        init_image = data['init_image']
        mask_image = mask_image['mask_image']
        guidance_scale = data['guidance_scale']
        eta = data['eta']
        num_inference_steps = data['num_inference_steps']
        do_loop = data['do_loop']
        make_video = data['make_video']
        use_lerp_for_text = data['use_lerp_for_text']
        scheduler = data['scheduler']
        disable_tqdm=disable_tqdm
        upsample = data['upsample'] if 'upsample' in data else upsample
        fps = data['fps'] if 'fps' in data else fps

        resume_step = int(sorted(output_path.glob(f"frame*{frame_filename_ext}"))[-1].stem[5:])
        print(f"\nResuming {output_path} from step {resume_step}...")


    pipeline.set_progress_bar_config(disable=disable_tqdm)
    pipeline.scheduler = SCHEDULERS[scheduler]

    assert len(prompts) == len(seeds)

    first_prompt, *prompts = prompts
    embeds_a = pipeline.embed_text(first_prompt)

    first_seed, *seeds = seeds
    latents_a = torch.randn(
        (1, pipeline.unet.in_channels, height // 8, width // 8),
        device=pipeline.device,
        generator=torch.Generator(device=pipeline.device).manual_seed(first_seed),
    )

    if do_loop:
        prompts.append(first_prompt)
        seeds.append(first_seed)

    frame_index = 0
    for prompt, seed in zip(prompts, seeds):
        # Text
        embeds_b = pipeline.embed_text(prompt)

        # Latent Noise
        latents_b = torch.randn(
            (1, pipeline.unet.in_channels, height // 8, width // 8),
            device=pipeline.device,
            generator=torch.Generator(device=pipeline.device).manual_seed(seed),
        )

        latents_batch, embeds_batch = None, None
        for i, t in enumerate(np.linspace(0, 1, num_steps)):

            frame_filepath = output_path / (f"frame%06d{frame_filename_ext}" % frame_index)
            if resume and frame_filepath.is_file():
                frame_index += 1
                continue

            if use_lerp_for_text:
                embeds = torch.lerp(embeds_a, embeds_b, float(t))
            else:
                embeds = slerp(float(t), embeds_a, embeds_b)
            latents = slerp(float(t), latents_a, latents_b)

            embeds_batch = embeds if embeds_batch is None else torch.cat([embeds_batch, embeds])
            latents_batch = latents if latents_batch is None else torch.cat([latents_batch, latents])

            del embeds
            del latents
            torch.cuda.empty_cache()

            batch_is_ready = embeds_batch.shape[0] == batch_size or t == 1.0
            if not batch_is_ready:
                continue

            do_print_progress = (i == 0) or ((frame_index) % 20 == 0)
            if do_print_progress:
                print(f"COUNT: {frame_index}/{len(seeds)*num_steps}")

            with torch.autocast("cuda"):
                outputs = pipeline(
                    latents=latents_batch,
                    text_embeddings=embeds_batch,
                    height=height,
                    width=width,
                    init_image=init_image,
                    mask_image=mask_image,
                    guidance_scale=guidance_scale,
                    eta=eta,
                    num_inference_steps=num_inference_steps,
                    output_type='pil' if not upsample else 'numpy'
                )["sample"]

                del embeds_batch
                del latents_batch
                torch.cuda.empty_cache()
                latents_batch, embeds_batch = None, None

                if upsample:
                    images = []
                    for output in outputs:
                        images.append(upsampling_pipeline(output))
                else:
                    images = outputs
            for image in images:
                frame_filepath = output_path / (f"frame%06d{frame_filename_ext}" % frame_index)
                image.save(frame_filepath)
                frame_index += 1

        embeds_a = embeds_b
        latents_a = latents_b

    if make_video:
        return make_video_ffmpeg(output_path, f"{name}.mp4", fps=fps, frame_filename=f"frame%06d{frame_filename_ext}")