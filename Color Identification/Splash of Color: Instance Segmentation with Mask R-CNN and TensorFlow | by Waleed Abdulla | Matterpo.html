<!doctype html><html lang="en"><head><title data-rh="true">Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow | by Waleed Abdulla | Matterport Engineering Techblog</title><meta data-rh="true" charset="utf-8"/><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1"/><meta data-rh="true" name="theme-color" content="#000000"/><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"/><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"/><meta data-rh="true" property="al:ios:app_name" content="Medium"/><meta data-rh="true" property="al:ios:app_store_id" content="828256236"/><meta data-rh="true" property="al:android:package" content="com.medium.reader"/><meta data-rh="true" property="fb:app_id" content="542599432471018"/><meta data-rh="true" property="og:site_name" content="Medium"/><meta data-rh="true" property="og:type" content="article"/><meta data-rh="true" property="article:published_time" content="2018-12-10T01:53:20.068Z"/><meta data-rh="true" name="title" content="Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow | by Waleed Abdulla | Matterport Engineering Techblog"/><meta data-rh="true" property="og:title" content="Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow"/><meta data-rh="true" property="twitter:title" content="Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow"/><meta data-rh="true" name="twitter:site" content="@matterport"/><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/7c761e238b46"/><meta data-rh="true" property="al:android:url" content="medium://p/7c761e238b46"/><meta data-rh="true" property="al:ios:url" content="medium://p/7c761e238b46"/><meta data-rh="true" property="al:android:app_name" content="Medium"/><meta data-rh="true" name="description" content="Back in November, we open-sourced our implementation of Mask R-CNN, and since then it’s been forked 1400 times, used in a lot of projects, and improved upon by many generous contributors. We received…"/><meta data-rh="true" property="og:description" content="Explained by building a color splash filter"/><meta data-rh="true" property="twitter:description" content="Explained by building a color splash filter"/><meta data-rh="true" property="og:url" content="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46"/><meta data-rh="true" property="al:web:url" content="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46"/><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/937/1*w_ownWZZ38QhiVjVU757DA.png"/><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/937/1*w_ownWZZ38QhiVjVU757DA.png"/><meta data-rh="true" name="twitter:card" content="summary_large_image"/><meta data-rh="true" property="article:author" content="https://medium.com/@waleedka"/><meta data-rh="true" name="twitter:creator" content="@waleedka"/><meta data-rh="true" name="author" content="Waleed Abdulla"/><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"/><meta data-rh="true" name="referrer" content="unsafe-url"/><meta data-rh="true" name="twitter:label1" content="Reading time"/><meta data-rh="true" name="twitter:data1" content="12 min read"/><link data-rh="true" rel="icon" href="https://miro.medium.com/fit/c/256/256/"/><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="/osd.xml"/><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"/><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"/><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="https://glyph.medium.com/css/unbound.css"/><link data-rh="true" rel="author" href="https://medium.com/@waleedka"/><link data-rh="true" rel="canonical" href="https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46"/><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/7c761e238b46"/><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*w_ownWZZ38QhiVjVU757DA.png"],"url":"https:\u002F\u002Fengineering.matterport.com\u002Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46","dateCreated":"2018-03-20T00:23:01.092Z","datePublished":"2018-03-20T00:23:01.092Z","dateModified":"2018-12-10T01:53:20.068Z","headline":"Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow","name":"Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow","description":"Back in November, we open-sourced our implementation of Mask R-CNN, and since then it’s been forked 1400 times, used in a lot of projects, and improved upon by many generous contributors. We received…","identifier":"7c761e238b46","author":{"@type":"Person","name":"Waleed Abdulla","url":"https:\u002F\u002Fengineering.matterport.com\u002F@waleedka"},"creator":["Waleed Abdulla"],"publisher":{"@type":"Organization","name":"Matterport Engineering Techblog","url":"engineering.matterport.com","logo":{"@type":"ImageObject","width":147,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F294\u002F1*g2BYfefpbBmRV26SMbaUxQ.png"}},"mainEntityOfPage":"https:\u002F\u002Fengineering.matterport.com\u002Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46"}</script><style type="text/css" data-fela-rehydration="588" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}#speechify-root{font-family:Sohne, sans-serif}div[data-popper-reference-hidden="true"]{visibility:hidden;pointer-events:none}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{display:block}.m{margin:auto}.n{max-width:1504px}.o{display:flex}.u{justify-content:space-between}.ag{height:100%}.al{padding:0 24px}.am{box-shadow:0px -2px 10px rgba(0, 0, 0, 0.15)}.an{height:56px}.ao{align-items:center}.ap{position:fixed}.aq{top:0}.ar{right:0}.as{left:0}.at{z-index:500}.au{color:inherit}.av{fill:inherit}.aw{font-size:inherit}.ax{border:inherit}.ay{font-family:inherit}.az{letter-spacing:inherit}.ba{font-weight:inherit}.bb{padding:0}.bc{margin:0}.bg:disabled{cursor:default}.bh:disabled{color:rgba(117, 117, 117, 1)}.bi:disabled{fill:rgba(117, 117, 117, 1)}.bj{height:25px}.bk{fill:rgba(41, 41, 41, 1)}.bl{padding-top:0px}.bm{text-align:center}.bn{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bo{font-size:14px}.bp{line-height:20px}.bq{color:rgba(242, 242, 242, 1)}.br{padding:7px 16px 9px}.bs{fill:rgba(242, 242, 242, 1)}.bt{background:rgba(242, 242, 242, 1)}.bu{border-color:rgba(242, 242, 242, 1)}.ca:disabled{cursor:inherit !important}.cb:disabled{opacity:0.1}.cc:disabled:hover{background:rgba(25, 25, 25, 1)}.cd:disabled:hover{border-color:rgba(25, 25, 25, 1)}.ce{border-radius:99em}.cf{width:100%}.cg{border-width:1px}.ch{border-style:solid}.ci{box-sizing:border-box}.cj{display:inline-block}.ck{text-decoration:none}.cl{margin-left:16px}.cm{display:none}.co{color:rgba(117, 117, 117, 1)}.cp{color:rgba(26, 137, 23, 1)}.cq{fill:rgba(26, 137, 23, 1)}.ct:disabled{color:rgba(163, 208, 162, 0.5)}.cu:disabled{fill:rgba(163, 208, 162, 0.5)}.da{height:100vh}.db{flex-direction:column}.dc{position:sticky}.dd{height:23px}.de{padding-bottom:35px}.df{fill:rgba(117, 117, 117, 1)}.dg{padding-left:28px}.dh{transition:all 0.2s ease-in-out}.dl{margin-right:28px}.dn{font-size:16px}.do{line-height:24px}.dp{position:relative}.dq{margin:0px 0px 35px 28px }.dr{width:24px}.ds{border:0}.dt{height:1px}.du{background-color:rgba(230, 230, 230, 1)}.dv{padding:0 24px 24px}.dw{height:64px}.dx path{fill:rgba(168, 168, 168, 1)}.dy{justify-content:center}.dz{flex:1}.ea{border:none}.eb{background:transparent}.ec{box-shadow:0px 2px 10px rgba(0, 0, 0, 0.15)}.ed{z-index:600}.ee{bottom:0}.ef{justify-content:space-around}.eg{height:16px}.eh{background-color:rgba(237, 237, 237, 1)}.en{min-width:0}.eo{flex:1 1 auto}.ep{padding:0 32px}.eq{border-left:1px solid rgba(230, 230, 230, 1)}.er{min-height:100vh}.es{width:394px}.eu{color:rgba(102, 139, 162, 1)}.ev{fill:rgba(102, 139, 162, 1)}.ey:disabled{color:rgba(102, 139, 162, 0.5)}.ez:disabled{fill:rgba(102, 139, 162, 0.5)}.fa{border-bottom:1px solid rgba(230, 230, 230, 1)}.fk{margin-right:16px}.fl{box-shadow:inset 0 0 0 1px rgba(0, 0, 0, 0.05)}.fm{border-radius:50%}.fn{height:32px}.fo{width:32px}.fp{position:absolute}.fq{background-color:rgba(242, 242, 242, 1)}.fr{margin-right:3px}.fs{flex:0 0 auto}.ft{overflow:hidden}.fu{max-height:20px}.fv{text-overflow:ellipsis}.fw{display:-webkit-box}.fx{-webkit-line-clamp:1}.fy{-webkit-box-orient:vertical}.fz{word-break:break-all}.gb{color:rgba(41, 41, 41, 1)}.gm{margin-left:auto}.gn{margin-right:auto}.go{max-width:728px}.gz{align-items:flex-start}.ha{height:48px}.hb{width:48px}.hc{margin-bottom:4px}.hd{flex-direction:row}.he{padding-left:12px}.hj{font-size:13px}.hk{color:rgba(255, 255, 255, 1)}.hl{padding:0px 8px 1px}.hm{fill:rgba(255, 255, 255, 1)}.hn{background:rgba(102, 139, 162, 1)}.ho{border-color:rgba(102, 139, 162, 1)}.hr:disabled{opacity:0.3}.hs:disabled:hover{background:rgba(102, 139, 162, 1)}.ht:disabled:hover{border-color:rgba(102, 139, 162, 1)}.hu{flex-wrap:wrap}.hv{padding:0 8px}.hz{padding-right:4px}.ia{padding:8px 2px}.ic{margin:0 4px 0 28px}.id path{fill:rgba(41, 41, 41, 1)}.ie{cursor:pointer}.ih svg path{fill:rgba(117, 117, 117, 1)}.ii{display:inline-flex}.ij{padding-top:24px}.im{border:1px solid rgba(230, 230, 230, 1)}.in{padding:6px 15px 6px 10px}.ip svg{margin-right:8px}.iq{padding-right:12px}.ir{background:rgba(255, 255, 255, 1)}.is{border-radius:4px}.it{box-shadow:0 1px 4px rgba(230, 230, 230, 1)}.iu{max-height:100vh}.iv{overflow-y:auto}.iw{top:calc(100vh + 100px)}.ix{bottom:calc(100vh + 100px)}.iy{width:10px}.iz{pointer-events:none}.ja{word-break:break-word}.jb{word-wrap:break-word}.jc:after{display:block}.jd:after{content:""}.je:after{clear:both}.jf{line-height:1.23}.jg{letter-spacing:0}.jh{font-style:normal}.ji{font-weight:700}.kd{margin-bottom:-0.27em}.ke{line-height:1.394}.kf{font-size:24px}.kv{margin-bottom:-0.42em}.kw{line-height:1.58}.kx{letter-spacing:-0.004em}.ky{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.lr{margin-bottom:-0.46em}.ls{text-decoration:underline}.lt{box-shadow:inset 3px 0 0 0 rgba(41, 41, 41, 1)}.lu{padding-left:23px}.lv{margin-left:-20px}.lw{font-style:italic}.ma{line-height:1.31}.mb{letter-spacing:-0.022em}.mc{font-weight:600}.mr{margin-bottom:-0.37em}.mx{max-width:996px}.nc{clear:both}.ne{cursor:zoom-in}.nf{z-index:auto}.nh{max-width:100%}.ni{height:auto}.nj{line-height:28px}.nk{letter-spacing:-0.003em}.no{list-style-type:disc}.np{margin-left:30px}.nq{padding-left:0px}.nr{font-size:18px}.nx{max-width:1285px}.ny{margin-top:10px}.ob{max-width:309px}.oc{line-height:1.18}.op{margin-bottom:-0.31em}.oq{max-width:452px}.or{max-width:593px}.os{max-width:407px}.ot{list-style-type:decimal}.ou{max-width:933px}.ov{max-width:645px}.ow{max-width:455px}.ox{max-width:460px}.oy{max-width:871px}.oz{max-width:948px}.pa{padding:2px 4px}.pb{font-size:75%}.pc> strong{font-family:inherit}.pd{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.pe{padding:20px}.pf{overflow-x:auto}.pg{margin-top:-0.09em}.ph{margin-bottom:-0.09em}.pi{white-space:pre-wrap}.po{max-width:831px}.pp{max-width:812px}.pq{max-width:942px}.pu{max-width:937px}.pv{padding:16px 0 0}.pw{border-top:none}.px{height:52px}.py{max-height:52px}.pz{box-sizing:content-box}.qa{position:static}.qb{z-index:1}.qc{flex:1 0 auto}.qe{max-width:155px}.qh{margin-right:5px}.qk{-webkit-user-select:none}.qn{outline:0}.qo{user-select:none}.qp> svg{pointer-events:none}.qy{cursor:progress}.rb{margin-left:24px}.rc{margin-top:0px}.rd{padding:4px 0}.rf{margin-left:4px}.rg{opacity:1}.rh path{fill:rgba(117, 117, 117, 1)}.rj{margin:0 20px}.rk{background-color:rgba(250, 250, 250, 1)}.rl{padding-bottom:4px}.rm{padding-top:32px}.rn{font-weight:500}.rt{padding-top:5px}.ru{padding-top:25px}.rv{padding-bottom:96px}.rw{padding-top:40px}.rx{padding-bottom:80px}.ry{padding-bottom:26px}.ti{flex-grow:0}.tj{padding-bottom:8px}.tk{margin-bottom:24px}.tl{margin-right:24px}.tm{flex:1 0 0%}.tn{margin-bottom:8px}.to{margin-right:8px}.tp{height:20px}.tq{width:20px}.tr{max-height:60px}.ts{-webkit-line-clamp:3}.tt{width:56px}.tu{padding-bottom:100%}.tv{height:0}.tw{border-radius:2px}.tx{padding:30px 0}.ty{margin-bottom:0}.tz{min-width:100vw}.ua{background-color:rgba(0, 0, 0, 1)}.uf{max-width:1192px}.ui:disabled{color:rgba(255, 255, 255, 0.6)}.uj:disabled{fill:rgba(255, 255, 255, 0.45)}.uk{height:22px}.ul{margin-top:20px}.um{color:rgba(255, 255, 255, 0.95)}.uo{margin-right:20px}.up{background-color:rgba(255, 255, 255, 0.4)}.uq{margin:28px 0 20px}.ur{padding:40px 0}.us{border-radius:20px}.ut{width:inherit}.uu{outline:none}.uv{padding:8px 0 11px}.uw{background-color:transparent}.ux::placeholder{color:rgba(117, 117, 117, 1)}.uy{padding:7px 7px 6px 8px}.uz{margin-top:40px}.va{height:88px}.vb{width:88px}.vc{margin-top:16px}.vd{margin-top:4px}.ve{margin-top:12px}.vf{margin-top:24px}.vg{margin-bottom:40px}.vh{width:auto}.vi{margin-left:8px}.vj{stroke:rgba(242, 242, 242, 1)}.vk{height:36px}.vl{width:36px}.vm{margin-top:32px}.vn{padding:24px 0}.vo{margin-right:6px}.vp{font-size:11px}.vq{line-height:16px}.bd:hover{cursor:pointer}.be:hover{color:rgba(25, 25, 25, 1)}.bf:hover{fill:rgba(25, 25, 25, 1)}.bv:hover{background:rgba(242, 242, 242, 1)}.bw:hover{border-color:rgba(242, 242, 242, 1)}.bx:hover{cursor:wait}.by:hover{color:rgba(242, 242, 242, 1)}.bz:hover{fill:rgba(242, 242, 242, 1)}.cr:hover{color:rgba(15, 115, 12, 1)}.cs:hover{fill:rgba(15, 115, 12, 1)}.di:hover{color:rgba(41, 41, 41, 1)}.dj:hover{fill:rgba(41, 41, 41, 1)}.ew:hover{color:rgba(90, 119, 138, 1)}.ex:hover{fill:rgba(90, 119, 138, 1)}.hp:hover{background:rgba(90, 119, 138, 1)}.hq:hover{border-color:rgba(90, 119, 138, 1)}.ib:hover path{fill:rgba(8, 8, 8, 1)}.if:hover svg path{fill:rgba(8, 8, 8, 1)}.io:hover{border-color:rgba(204, 204, 204, 1)}.qm:hover{fill:rgba(8, 8, 8, 1)}.re:hover p{color:rgba(8, 8, 8, 1)}.ug:hover{color:rgba(255, 255, 255, 1)}.uh:hover{fill:rgba(255, 255, 255, 0.9)}.un:hover{text-decoration:underline}.ig:focus svg path{fill:rgba(8, 8, 8, 1)}.ng:focus{transform:scale(1.01)}.ql:focus{fill:rgba(8, 8, 8, 1)}.ri:focus path{fill:rgba(8, 8, 8, 1)}.qq:active{border-style:none}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.t{flex-direction:row}.z{width:80px}.ab{min-height:100vh}.ac{flex-shrink:1}.ae{border-right:1px solid rgba(230, 230, 230, 1)}.cv{display:block}.cw{text-align:center}.cx{padding:40px 0}.em{margin-bottom:0}.fh{margin:0 32px}.fi{max-width:692px}.gg{margin-bottom:40px}.gl{padding:0 16px}.gx{margin-bottom:32px}.gy{margin-top:56px}.hy{display:inline-flex}.jz{font-size:32px}.ka{margin-top:0.6em}.kb{line-height:40px}.kc{letter-spacing:-0.016em}.ks{margin-top:0.79em}.kt{font-size:22px}.ku{line-height:28px}.ln{font-size:20px}.lo{margin-top:2em}.lp{line-height:32px}.lq{letter-spacing:-0.003em}.lz{font-size:21px}.mp{margin-top:3.14em}.mq{letter-spacing:0}.mw{margin-top:0.86em}.nn{margin-top:2.14em}.nw{margin-top:1.14em}.on{margin-top:2.37em}.oo{line-height:24px}.pn{margin-top:1.91em}.pt{margin-top:0.94em}.qx{margin-top:0px}.ra{display:inline-block}.rs{max-height:24px}.sl{width:calc(100% + 64px)}.sm{margin-left:-32px}.sn{margin-right:-32px}.te{padding-left:32px}.tf{padding-right:32px}.tg{flex-basis:50%}.th{max-width:50%}.ue{margin:0 64px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.nz{margin-left:auto}.oa{text-align:center}.qw{margin-top:0px}.qz{display:inline-block}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.qg{display:inline-block}.qv{margin-top:0px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.cn{display:block}.qf{display:inline-block}.qt{margin-top:0px}.qu{margin-right:0px}.ub{padding:24px 0}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.p{flex-direction:column}.v{width:auto}.ah{display:block}.ei{margin-bottom:56px}.fb{margin:0 24px}.gc{margin-bottom:80px}.gh{padding:0 8px}.gp{margin-bottom:24px}.gq{margin-top:32px}.hf{display:inline-block}.ik{display:flex}.jj{font-size:32px}.jk{margin-top:0.64em}.jl{line-height:40px}.jm{letter-spacing:-0.016em}.kg{margin-top:0.46em}.kh{font-size:22px}.ki{line-height:28px}.kz{font-size:18px}.la{margin-top:1.56em}.lb{letter-spacing:-0.003em}.md{font-size:20px}.me{margin-top:1.9em}.mf{line-height:24px}.mg{letter-spacing:0}.ms{margin-top:0.67em}.my{margin-top:40px}.ns{margin-top:1.34em}.od{font-size:16px}.oe{margin-top:2.07em}.of{line-height:20px}.pj{margin-top:1.41em}.qi{margin-left:0px}.qr{margin-top:0px}.qs{margin-right:0px}.ro{max-height:20px}.rz{width:calc(100% + 24px)}.sa{margin-left:-12px}.sb{margin-right:-12px}.so{padding-left:12px}.sp{padding-right:12px}.sq{flex-basis:100%}.sr{max-width:100%}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.s{flex-direction:column}.y{width:auto}.ak{display:block}.el{margin-bottom:56px}.ff{margin:0 32px}.fg{max-width:692px}.gf{margin-bottom:40px}.gk{padding:0 16px}.gv{margin-bottom:24px}.gw{margin-top:32px}.hi{display:inline-block}.hx{display:inline-flex}.jv{font-size:32px}.jw{margin-top:0.6em}.jx{line-height:40px}.jy{letter-spacing:-0.016em}.kp{margin-top:0.79em}.kq{font-size:22px}.kr{line-height:28px}.lj{font-size:20px}.lk{margin-top:2em}.ll{line-height:32px}.lm{letter-spacing:-0.003em}.ly{font-size:21px}.mn{margin-top:3.14em}.mo{letter-spacing:0}.mv{margin-top:0.86em}.nb{margin-top:56px}.nm{margin-top:2.14em}.nv{margin-top:1.14em}.ol{margin-top:2.37em}.om{line-height:24px}.pm{margin-top:1.91em}.ps{margin-top:0.94em}.rr{max-height:24px}.si{width:calc(100% + 64px)}.sj{margin-left:-32px}.sk{margin-right:-32px}.ta{padding-left:32px}.tb{padding-right:32px}.tc{flex-basis:50%}.td{max-width:50%}.ud{margin:0 64px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.r{flex-direction:column}.x{width:auto}.aj{display:block}.ek{margin-bottom:56px}.fd{margin:0 32px}.fe{max-width:692px}.ge{margin-bottom:40px}.gj{padding:0 16px}.gt{margin-bottom:24px}.gu{margin-top:32px}.hh{display:inline-block}.hw{display:inline-flex}.jr{font-size:32px}.js{margin-top:0.6em}.jt{line-height:40px}.ju{letter-spacing:-0.016em}.km{margin-top:0.79em}.kn{font-size:22px}.ko{line-height:28px}.lf{font-size:20px}.lg{margin-top:2em}.lh{line-height:32px}.li{letter-spacing:-0.003em}.lx{font-size:21px}.ml{margin-top:3.14em}.mm{letter-spacing:0}.mu{margin-top:0.86em}.na{margin-top:56px}.nl{margin-top:2.14em}.nu{margin-top:1.14em}.oj{margin-top:2.37em}.ok{line-height:24px}.pl{margin-top:1.91em}.pr{margin-top:0.94em}.rq{max-height:24px}.sf{width:calc(100% + 64px)}.sg{margin-left:-32px}.sh{margin-right:-32px}.sw{padding-left:32px}.sx{padding-right:32px}.sy{flex-basis:50%}.sz{max-width:50%}.uc{margin:0 48px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.q{flex-direction:column}.w{width:auto}.ai{display:block}.ej{margin-bottom:56px}.fc{margin:0 24px}.gd{margin-bottom:80px}.gi{padding:0 8px}.gr{margin-bottom:24px}.gs{margin-top:32px}.hg{display:inline-block}.il{display:flex}.jn{font-size:32px}.jo{margin-top:0.64em}.jp{line-height:40px}.jq{letter-spacing:-0.016em}.kj{margin-top:0.46em}.kk{font-size:22px}.kl{line-height:28px}.lc{font-size:18px}.ld{margin-top:1.56em}.le{letter-spacing:-0.003em}.mh{font-size:20px}.mi{margin-top:1.9em}.mj{line-height:24px}.mk{letter-spacing:0}.mt{margin-top:0.67em}.mz{margin-top:40px}.nt{margin-top:1.34em}.og{font-size:16px}.oh{margin-top:2.07em}.oi{line-height:20px}.pk{margin-top:1.41em}.qj{margin-left:0px}.rp{max-height:20px}.sc{width:calc(100% + 64px)}.sd{margin-left:-32px}.se{margin-right:-32px}.ss{padding-left:32px}.st{padding-right:32px}.su{flex-basis:50%}.sv{max-width:50%}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="print">.qd{display:none}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (min-width: 7000px)">.af{width:224px}.cy{text-align:left}.cz{padding:40px 24px}.dk{display:none}.dm{display:block}.fj{margin:0 auto}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="all and (max-width: 1239.98px)">.et{width:280px}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.ga{max-height:none}</style><style type="text/css" data-fela-rehydration="588" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.nd{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}</style></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="l c"><div class="m n l"><div class="o p q r s t u"><div class="v w x y z ab ac ae af"><nav class="ag"><div class="ah ai aj ak d"><div class="al am an o ao u ap aq ar as at c"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" aria-label="Homepage" href="https://medium.com/" rel="noopener follow"><svg viewBox="0 0 1043.63 592.71" class="bj bk"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a><div class="o ao"><div class="cl cm cn"><span class="bn b bo bp co"><a class="eu ev aw ax ay az ba bb bc bd ew ex bg ey ez" href="https://rsci.app.link/?$canonical_url=https%3A%2F%2Fmedium.com/p/7c761e238b46&amp;~feature=LoOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar" rel="noopener follow">Open in app</a></span></div></div></div><div class="an l"></div></div><div class="ag h k j i cv"><div class="da o db u dc aq at c"><div class="cw cx cy cz"><a aria-label="Homepage" href="https://medium.com/" rel="noopener follow"><svg viewBox="0 0 1043.63 592.71" class="dd bk"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div><div class="l"><div class="de l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/" rel="noopener follow"><div class="o dp"><div class="co df o dg dh di dj"><div class="l dk dl"><div><div class="cj" aria-hidden="false"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Home"><path d="M4.5 10.75v10.5c0 .14.11.25.25.25h5c.14 0 .25-.11.25-.25v-5.5c0-.14.11-.25.25-.25h3.5c.14 0 .25.11.25.25v5.5c0 .14.11.25.25.25h5c.14 0 .25-.11.25-.25v-10.5M22 9l-9.1-6.83a1.5 1.5 0 0 0-1.8 0L2 9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path></svg></div></div></div><div class="cm dm dl" aria-hidden="true"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Home"><path d="M4.5 10.75v10.5c0 .14.11.25.25.25h5c.14 0 .25-.11.25-.25v-5.5c0-.14.11-.25.25-.25h3.5c.14 0 .25.11.25.25v5.5c0 .14.11.25.25.25h5c.14 0 .25-.11.25-.25v-10.5M22 9l-9.1-6.83a1.5 1.5 0 0 0-1.8 0L2 9" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round"></path></svg></div><div class="cm dm bn b dn do">Home</div></div></div></a></div><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fme%2Fnotifications&amp;source=--------------------------notifications_sidenav-----------" rel="noopener follow"><div class="de l"><div class="o dp"><div class="co df o dg dh di dj"><div class="l dk dl"><div><div class="cj" aria-hidden="false"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Notifications"><path d="M15 18.5a3 3 0 1 1-6 0" stroke="currentColor" stroke-linecap="round"></path><path d="M5.5 10.53V9a6.5 6.5 0 0 1 13 0v1.53c0 1.42.56 2.78 1.57 3.79l.03.03c.26.26.4.6.4.97v2.93c0 .14-.11.25-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.93c0-.37.14-.71.4-.97l.03-.03c1-1 1.57-2.37 1.57-3.79z" stroke="currentColor" stroke-linejoin="round"></path></svg></div></div></div><div class="cm dm dl" aria-hidden="true"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Notifications"><path d="M15 18.5a3 3 0 1 1-6 0" stroke="currentColor" stroke-linecap="round"></path><path d="M5.5 10.53V9a6.5 6.5 0 0 1 13 0v1.53c0 1.42.56 2.78 1.57 3.79l.03.03c.26.26.4.6.4.97v2.93c0 .14-.11.25-.25.25H3.75a.25.25 0 0 1-.25-.25v-2.93c0-.37.14-.71.4-.97l.03-.03c1-1 1.57-2.37 1.57-3.79z" stroke="currentColor" stroke-linejoin="round"></path></svg></div><div class="cm dm bn b dn do">Notifications</div></div></div></div></a></span><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fme%2Flists&amp;source=--------------------------lists_sidenav-----------" rel="noopener follow"><div class="de l"><div class="o dp"><div class="co df o dg dh di dj"><div class="l dk dl"><div><div class="cj" aria-hidden="false"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Lists"><path d="M4.5 6.25V21c0 .2.24.32.4.2l5.45-4.09a.25.25 0 0 1 .3 0l5.45 4.09c.16.12.4 0 .4-.2V6.25a.25.25 0 0 0-.25-.25H4.75a.25.25 0 0 0-.25.25z" stroke="currentColor" stroke-linecap="round"></path><path d="M8 6V3.25c0-.14.11-.25.25-.25h11.5c.14 0 .25.11.25.25V16.5" stroke="currentColor" stroke-linecap="round"></path></svg></div></div></div><div class="cm dm dl" aria-hidden="true"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Lists"><path d="M4.5 6.25V21c0 .2.24.32.4.2l5.45-4.09a.25.25 0 0 1 .3 0l5.45 4.09c.16.12.4 0 .4-.2V6.25a.25.25 0 0 0-.25-.25H4.75a.25.25 0 0 0-.25.25z" stroke="currentColor" stroke-linecap="round"></path><path d="M8 6V3.25c0-.14.11-.25.25-.25h11.5c.14 0 .25.11.25.25V16.5" stroke="currentColor" stroke-linecap="round"></path></svg></div><div class="cm dm bn b dn do">Lists</div></div></div></div></a></span><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fme%2Fstories%2Fdrafts&amp;source=--------------------------stories_sidenav-----------" rel="noopener follow"><div class="de l"><div class="o dp"><div class="co df o dg dh di dj"><div class="l dk dl"><div><div class="cj" aria-hidden="false"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Stories"><path d="M4.75 21.5h14.5c.14 0 .25-.11.25-.25V2.75a.25.25 0 0 0-.25-.25H4.75a.25.25 0 0 0-.25.25v18.5c0 .14.11.25.25.25z" stroke="currentColor"></path><path d="M8 8.5h8M8 15.5h5M8 12h8" stroke="currentColor" stroke-linecap="round"></path></svg></div></div></div><div class="cm dm dl" aria-hidden="true"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Stories"><path d="M4.75 21.5h14.5c.14 0 .25-.11.25-.25V2.75a.25.25 0 0 0-.25-.25H4.75a.25.25 0 0 0-.25.25v18.5c0 .14.11.25.25.25z" stroke="currentColor"></path><path d="M8 8.5h8M8 15.5h5M8 12h8" stroke="currentColor" stroke-linecap="round"></path></svg></div><div class="cm dm bn b dn do">Stories</div></div></div></div></a></span><div class="dq dr l"><hr class="ds dt du bc" aria-hidden="true"/></div><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fmedium.com%2Fnew-story&amp;source=--------------------------new_post_sidenav-----------" rel="noopener follow"><div class="de l"><div class="o dp"><div class="co df o dg dh di dj"><div class="l dk dl"><div><div class="cj" aria-hidden="false"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Write"><path d="M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z" fill="currentColor"></path><path d="M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2" stroke="currentColor"></path></svg></div></div></div><div class="cm dm dl" aria-hidden="true"><svg width="24" height="24" viewBox="0 0 24 24" fill="none" aria-label="Write"><path d="M14 4a.5.5 0 0 0 0-1v1zm7 6a.5.5 0 0 0-1 0h1zm-7-7H4v1h10V3zM3 4v16h1V4H3zm1 17h16v-1H4v1zm17-1V10h-1v10h1zm-1 1a1 1 0 0 0 1-1h-1v1zM3 20a1 1 0 0 0 1 1v-1H3zM4 3a1 1 0 0 0-1 1h1V3z" fill="currentColor"></path><path d="M17.5 4.5l-8.46 8.46a.25.25 0 0 0-.06.1l-.82 2.47c-.07.2.12.38.31.31l2.47-.82a.25.25 0 0 0 .1-.06L19.5 6.5m-2-2l2.32-2.32c.1-.1.26-.1.36 0l1.64 1.64c.1.1.1.26 0 .36L19.5 6.5m-2-2l2 2" stroke="currentColor"></path></svg></div><div class="cm dm bn b dn do">Write</div></div></div></div></a></span></div><div class="dv dw o ao"></div></div></div><div class="ah ai aj ak d"><div class="l ap ar ee as at c"><div class="ec an dp ed"><div class="ag o ao ef"><div class="eg dr l eh"></div><div class="eg dr l eh"></div><div class="eg dr l eh"></div></div></div></div></div></nav></div><main class="ei ej ek el em en l eo"><div class="l"><div class="fa l"><div class="o dy"><div class="en cf fb fc fd fe ff fg fh fi fj"><div class="an o ao"><div class="fk l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://engineering.matterport.com/?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow"><div class="l dp"><img alt="Matterport Engineering Techblog" class="l ci fm fn fo fq" src="https://miro.medium.com/fit/c/64/64/1*bSmfQypBkGaCulaTyJk4ew.png" width="32" height="32"/><div class="fl fm l fn fo fp aq"></div></div></a></div><div class="fr l fs"><div class="bn b bo bp co">Published in</div></div><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://engineering.matterport.com/?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow"><p class="bn b bo bp ft fu fv fw fx fy fz ga gb">Matterport Engineering Techblog</p></a></div></div></div></div><div class="gc gd ge gf gg l"><div class="o dy"><div class="en cf fb fc fd fe ff fg fh fi fj"><article><div class="l"><div class="gh gi gj gk gl gm gn cf go ci l"></div><div class="l"><header class="pw-post-byline-header gp gq gr gs gt gu gv gw gx gy l"><div class="o gz u"><div class="o"><div class="fk l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@waleedka?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow"><div class="l dp"><img alt="Waleed Abdulla" class="l ci fm ha hb fq" src="https://miro.medium.com/fit/c/96/96/0*RDTSqB0ocRKRgYEA.JPG" width="48" height="48"/><div class="fl fm l ha hb fp aq"></div></div></a></div><div class="l"><div class="pw-author bn b dn do gb"><div class="hc o hd"><div><div class="cj" aria-hidden="false"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@waleedka?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow">Waleed Abdulla</a></div></div><div class="he hf hg hh hi d"><span><a class="bn b hj bp hk hl hm hn ho hp hq bd ca hr hs ht ce cg ch ci cj ck" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a69ae209bc4&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;user=Waleed+Abdulla&amp;userId=1a69ae209bc4&amp;source=post_page-1a69ae209bc4----7c761e238b46---------------------follow_byline-----------" rel="noopener follow">Follow</a></span></div></div></div><div class="o ao hu"><p class="pw-published-date bn b bo bp co"><span>Mar 20, 2018</span></p><div class="hv cj" aria-hidden="true"><span class="l" aria-hidden="true"><span class="bn b bo bp co">·</span></span></div><div class="pw-reading-time bn b bo bp co">12 min read</div></div></div></div><div class="o ao"><div class="h k hw hx hy"><div class="hz l fs"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi" aria-label="Share on twitter"><span class="cj ia dx ib"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="hz l fs"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi" aria-label="Share on facebook"><span class="cj ia dx ib"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="hz l fs"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi" aria-label="Share on linkedin"><span class="cj ia dx ib"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fs"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="cj ia dx ib"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="ic o ao"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7c761e238b46&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au df aw ax ay az ba ia bc ie if ig ih"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="id" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></a></span></div></div><div class="cl ii"><div><div class="cj" aria-hidden="false"></div></div></div></div></div><div class="ij ik il j i d"><div class="fk l"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7c761e238b46&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;source=--------------------------bookmark_header-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au df aw im ay az ba in bc ie ce o ao io ip ih"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="id" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg><p class="bn b bo bp co">Save</p></button></a></span></div><div class="iq l fs"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi" aria-label="Share on twitter"><span class="cj ia dx ib"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M20 5.34c-.67.41-1.4.7-2.18.87a3.45 3.45 0 0 0-5.02-.1 3.49 3.49 0 0 0-1.02 2.47c0 .28.03.54.07.8a9.91 9.91 0 0 1-7.17-3.66 3.9 3.9 0 0 0-.5 1.74 3.6 3.6 0 0 0 1.56 2.92 3.36 3.36 0 0 1-1.55-.44V10c0 1.67 1.2 3.08 2.8 3.42-.3.06-.6.1-.94.12l-.62-.06a3.5 3.5 0 0 0 3.24 2.43 7.34 7.34 0 0 1-4.36 1.49l-.81-.05a9.96 9.96 0 0 0 5.36 1.56c6.4 0 9.91-5.32 9.9-9.9v-.5c.69-.49 1.28-1.1 1.74-1.81-.63.3-1.3.48-2 .56A3.33 3.33 0 0 0 20 5.33" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="iq l fs"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi" aria-label="Share on facebook"><span class="cj ia dx ib"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M19.75 12.04c0-4.3-3.47-7.79-7.75-7.79a7.77 7.77 0 0 0-5.9 12.84 7.77 7.77 0 0 0 4.69 2.63v-5.49h-1.9v-2.2h1.9v-1.62c0-1.88 1.14-2.9 2.8-2.9.8 0 1.49.06 1.69.08v1.97h-1.15c-.91 0-1.1.43-1.1 1.07v1.4h2.17l-.28 2.2h-1.88v5.52a7.77 7.77 0 0 0 6.7-7.71" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="iq l fs"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi" aria-label="Share on linkedin"><span class="cj ia dx ib"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M19.75 5.39v13.22a1.14 1.14 0 0 1-1.14 1.14H5.39a1.14 1.14 0 0 1-1.14-1.14V5.39a1.14 1.14 0 0 1 1.14-1.14h13.22a1.14 1.14 0 0 1 1.14 1.14zM8.81 10.18H6.53v7.3H8.8v-7.3zM9 7.67a1.31 1.31 0 0 0-1.3-1.32h-.04a1.32 1.32 0 0 0 0 2.64A1.31 1.31 0 0 0 9 7.71v-.04zm8.46 5.37c0-2.2-1.4-3.05-2.78-3.05a2.6 2.6 0 0 0-2.3 1.18h-.07v-1h-2.14v7.3h2.28V13.6a1.51 1.51 0 0 1 1.36-1.63h.09c.72 0 1.26.45 1.26 1.6v3.91h2.28l.02-4.43z" fill="#A8A8A8"></path></svg></span></button></div></div></div><div class="l fs"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi"><span class="cj ia dx ib"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M3.57 14.67c0-.57.13-1.11.38-1.6l.02-.02v-.02l.02-.02c0-.02 0-.02.02-.02.12-.26.3-.52.57-.8L7.78 9v-.02l.01-.02c.44-.41.91-.7 1.44-.85a4.87 4.87 0 0 0-1.19 2.36A5.04 5.04 0 0 0 8 11.6L6.04 13.6c-.19.19-.32.4-.38.65a2 2 0 0 0 0 .9c.08.2.2.4.38.57l1.29 1.31c.27.28.62.42 1.03.42.42 0 .78-.14 1.06-.42l1.23-1.25.79-.78 1.15-1.16c.08-.09.19-.22.28-.4.1-.2.15-.42.15-.67 0-.16-.02-.3-.06-.45l-.02-.02v-.02l-.07-.14s0-.03-.04-.06l-.06-.13-.02-.02c0-.02 0-.03-.02-.05a.6.6 0 0 0-.14-.16l-.48-.5c0-.04.02-.1.04-.15l.06-.12 1.17-1.14.09-.09.56.57c.02.04.08.1.16.18l.05.04.03.06.04.05.03.04.04.06.1.14.02.02c0 .02.01.03.03.04l.1.2v.02c.1.16.2.38.3.68a1 1 0 0 1 .04.25 3.2 3.2 0 0 1 .02 1.33 3.49 3.49 0 0 1-.95 1.87l-.66.67-.97.97-1.56 1.57a3.4 3.4 0 0 1-2.47 1.02c-.97 0-1.8-.34-2.49-1.03l-1.3-1.3a3.55 3.55 0 0 1-1-2.51v-.01h-.02v.02zm5.39-3.43c0-.19.02-.4.07-.63.13-.74.44-1.37.95-1.87l.66-.67.97-.98 1.56-1.56c.68-.69 1.5-1.03 2.47-1.03.97 0 1.8.34 2.48 1.02l1.3 1.32a3.48 3.48 0 0 1 1 2.48c0 .58-.11 1.11-.37 1.6l-.02.02v.02l-.02.04c-.14.27-.35.54-.6.8L16.23 15l-.01.02-.01.02c-.44.42-.92.7-1.43.83a4.55 4.55 0 0 0 1.23-3.52L18 10.38c.18-.21.3-.42.35-.65a2.03 2.03 0 0 0-.01-.9 1.96 1.96 0 0 0-.36-.58l-1.3-1.3a1.49 1.49 0 0 0-1.06-.42c-.42 0-.77.14-1.06.4l-1.2 1.27-.8.8-1.16 1.15c-.08.08-.18.21-.29.4a1.66 1.66 0 0 0-.08 1.12l.02.03v.02l.06.14s.01.03.05.06l.06.13.02.02.01.02.01.02c.05.08.1.13.14.16l.47.5c0 .04-.02.09-.04.15l-.06.12-1.15 1.15-.1.08-.56-.56a2.3 2.3 0 0 0-.18-.19c-.02-.01-.02-.03-.02-.04l-.02-.02a.37.37 0 0 1-.1-.12c-.03-.03-.05-.04-.05-.06l-.1-.15-.02-.02-.02-.04-.08-.17v-.02a5.1 5.1 0 0 1-.28-.69 1.03 1.03 0 0 1-.04-.26c-.06-.23-.1-.46-.1-.7v.01z" fill="#A8A8A8"></path></svg></span></button></div></div></div></div></header><span class="l"></span><section><div><div class="fp as iw ix iy iz"></div><div class="ja jb jc jd je"><div class=""><h1 id="f392" class="pw-post-title jf jg jh bn ji jj jk jl jm jn jo jp jq jr js jt ju jv jw jx jy jz ka kb kc kd gb">Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow</h1></div><div class=""><h2 id="fcd0" class="pw-subtitle-paragraph ke jg jh bn b kf kg kh ki kj kk kl km kn ko kp kq kr ks kt ku kv co">Explained by building a color splash filter</h2></div><p id="04b2" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">Back in November, we open-sourced our <a class="au ls" href="https://github.com/matterport/Mask_RCNN" rel="noopener ugc nofollow" target="_blank">implementation of Mask R-CNN</a>, and since then it’s been forked 1400 times, used in a lot of projects, and improved upon by many generous contributors. We received a lot of questions as well, so in this post I’ll explain how the model works and show how to use it in a real application.</p><p id="64b6" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">I’ll cover two things: First, an overview of Mask RCNN. And, second, how to train a model from scratch and use it to build a smart color splash filter.</p><blockquote class="lt lu lv"><p id="3f65" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>We’re sharing the code <a class="au ls" href="https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon" rel="noopener ugc nofollow" target="_blank">here</a>. Including the dataset I built and the trained model. Follow along!</p></blockquote><h1 id="1d4c" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">What is Instance Segmentation?</h1><p id="4e7c" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">Instance segmentation is the task of identifying object outlines at the pixel level. Compared to similar computer vision tasks, it’s one of the hardest possible vision tasks. Consider the following asks:</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn mx"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*-zw_Mh1e-8YncnokbAFWxg.png" width="700" height="526" loading="lazy" role="presentation"/></div></div></figure><ul class=""><li id="beac" class="nj nk jh ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr gb"><strong class="ky ji">Classification: </strong>There is a balloon in this image.</li><li id="cec8" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><strong class="ky ji">Semantic Segmentation:</strong> These are all the balloon pixels.</li><li id="db7f" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><strong class="ky ji">Object Detection: </strong>There are 7 balloons in this image at these locations. We’re starting to account for objects that overlap.</li><li id="a421" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><strong class="ky ji">Instance Segmentation</strong>: There are 7 balloons at these locations, and these are the pixels that belong to each one.</li></ul><h1 id="fb1b" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">Mask R-CNN</h1><p id="4745" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">Mask R-CNN (regional convolutional neural network) is a two stage framework: the first stage scans the image and generates <em class="lw">proposals</em>(areas likely to contain an object). And the second stage classifies the proposals and generates bounding boxes and masks.</p><p id="7305" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">It was introduced last year via the <a class="au ls" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">Mask R-CNN paper</a> to extend its predecessor, <a class="au ls" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">Faster R-CNN</a>, by the same authors. Faster R-CNN is a popular framework for object detection, and Mask R-CNN extends it with instance segmentation, among other things.</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn nx"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*IWWOPIYLqqF9i_gXPmBk3g.png" width="700" height="316" loading="lazy" role="presentation"/></div></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">Mask R-CNN framework. Source: <a class="au ls" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1703.06870</a></figcaption></figure><p id="9820" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">At a high level, Mask R-CNN consists of these modules:</p><h1 id="5d32" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">1. Backbone</h1><figure class="my mz na nb gy nc gm gn paragraph-image"><div class="gm gn ob"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/618/1*IDjLXsSw5QMFWDudayIBfw.png" width="309" height="270" loading="lazy" role="presentation"/></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">Simplified illustration of the backbone nework</figcaption></figure><p id="b9da" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">This is a standard convolutional neural network (typically, ResNet50 or ResNet101) that serves as a feature extractor. The early layers detect low level features (edges and corners), and later layers successively detect higher level features (car, person, sky).</p><p id="5ab7" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">Passing through the backbone network, the image is converted from 1024x1024px x 3 (RGB) to a feature map of shape 32x32x2048. This feature map becomes the input for the following stages.</p><blockquote class="lt lu lv"><p id="b6fc" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>The backbone is built in the function <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L171" rel="noopener ugc nofollow" target="_blank">resnet_graph()</a>. The code supports ResNet50 and ResNet101.</p></blockquote><h2 id="62a8" class="oc mb jh bn mc od oe of mg og oh oi mk lf oj ok mm lj ol om mo ln on oo mq op gb">Feature Pyramid Network</h2><figure class="my mz na nb gy nc gm gn paragraph-image"><div class="gm gn oq"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/904/1*1sCveJrqfthOQsGGZRs2tQ.png" width="452" height="177" loading="lazy" role="presentation"/></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">Source: Feature Pyramid Networks paper</figcaption></figure><p id="3947" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">While the backbone described above works great, it can be improved upon. The <a class="au ls" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank">Feature Pyramid Network (FPN)</a> was introduced by the same authors of Mask R-CNN as an extension that can better represent objects at multiple scales.</p><p id="4b58" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">FPN improves the standard feature extraction pyramid by adding a second pyramid that takes the high level features from the first pyramid and passes them down to lower layers. By doing so, it allows features at every level to have access to both, lower and higher level features.</p><p id="2946" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">Our implementation of Mask RCNN uses a ResNet101 + FPN backbone.</p><blockquote class="lt lu lv"><p id="2994" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/> The FPN is created in <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L1840" rel="noopener ugc nofollow" target="_blank">MaskRCNN.build()</a>. The section after building the ResNet. <br/>RPN introduces additional complexity: rather than a single backbone feature map in the standard backbone (i.e. the top layer of the first pyramid), in FPN there is a feature map at each level of the second pyramid. We pick which to use dynamically depending on the size of the object. I’ll continue to refer to the <strong class="ky ji">backbone feature map</strong> as if it’s one feature map, but keep in mind that when using FPN, we’re actually picking one out of several at runtime.</p></blockquote><h1 id="1cdb" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">2. Region Proposal Network (RPN)</h1><figure class="my mz na nb gy nc gm gn paragraph-image"><div class="gm gn or"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1186/1*ESpJx0XLvyBa86TNo2BfLQ.png" width="593" height="578" loading="lazy" role="presentation"/></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">Simplified illustration showing 49 anchor boxes</figcaption></figure><p id="465d" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">The RPN is a lightweight neural network that scans the image in a sliding-window fashion and finds areas that contain objects.</p><p id="ab7f" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">The regions that the RPN scans over are called <em class="lw">anchors</em>. Which are boxes distributed over the image area, as show on the left. This is a simplified view, though. In practice, there are about 200K anchors of different sizes and aspect ratios, and they overlap to cover as much of the image as possible.</p><p id="8557" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">How fast can the RPN scan that many anchors? Pretty fast, actually. The sliding window is handled by the convolutional nature of the RPN, which allows it to scan all regions in parallel (on a GPU). Further, the RPN doesn’t scan over the image directly (even though we draw the anchors on the image for illustration). Instead, the RPN scans over the backbone feature map. This allows the RPN to reuse the extracted features efficiently and avoid duplicate calculations. With these optimizations, the RPN runs in about 10 ms according to the <a class="au ls" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">Faster RCNN paper</a> that introduced it. In Mask RCNN we typically use larger images and more anchors, so it might take a bit longer.</p><blockquote class="lt lu lv"><p id="dbe2" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:<br/></strong>The RPN is created in <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L831" rel="noopener ugc nofollow" target="_blank">rpn_graph()</a>. Anchor scales and aspect ratios are controlled by RPN_ANCHOR_SCALES and RPN_ANCHOR_RATIOS in <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/config.py" rel="noopener ugc nofollow" target="_blank">config.py</a>.</p></blockquote><p id="384b" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">The RPN generates two outputs for each anchor:</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div class="gm gn os"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/814/1*EMNE8bxOT4RI3HMjIqjCwQ.png" width="407" height="333" loading="lazy" role="presentation"/></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">3 anchor boxes (dotted) and the shift/scale applied to them to fit the object precisely (solid). Several anchors can map to the same object.</figcaption></figure><ol class=""><li id="e802" class="nj nk jh ky b kz la lc ld lf nl lj nm ln nn lr ot np nq nr gb"><strong class="ky ji">Anchor Class:</strong> One of two classes: foreground or background. The FG class implies that there is likely an object in that box.</li><li id="6e7e" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr ot np nq nr gb"><strong class="ky ji">Bounding Box Refinement:</strong> A foreground anchor (also called positive anchor) might not be centered perfectly over the object. So the RPN estimates a delta (% change in x, y, width, height) to refine the anchor box to fit the object better.</li></ol><p id="bb7a" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">Using the RPN predictions, we pick the top anchors that are likely to contain objects and refine their location and size. If several anchors overlap too much, we keep the one with the highest foreground score and discard the rest (referred to as Non-max Suppression). After that we have the final <em class="lw">proposals </em>(regions of interest)<em class="lw"> </em>that we pass to the next stage.</p><blockquote class="lt lu lv"><p id="4b8c" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>The <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L255" rel="noopener ugc nofollow" target="_blank">ProposalLayer</a> is a custom Keras layer that reads the output of the RPN, picks top anchors, and applies bounding box refinement.</p></blockquote><h1 id="1c01" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">3. ROI Classifier &amp; Bounding Box Regressor</h1><p id="cdfb" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">This stage runs on the regions of interest (ROIs) proposed by the RPN. And just like the RPN, it generates two outputs for each ROI:</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn ou"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*xQYuM_9mu5kt8nNN8Ms2TQ.png" width="700" height="268" loading="lazy" role="presentation"/></div></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">Illustration of stage 2. Source: Fast R-CNN (<a class="au ls" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">https://arxiv.org/abs/1504.08083</a>)</figcaption></figure><ol class=""><li id="64f4" class="nj nk jh ky b kz la lc ld lf nl lj nm ln nn lr ot np nq nr gb"><strong class="ky ji">Class:</strong> The class of the object in the ROI. Unlike the RPN, which has two classes (FG/BG), this network is deeper and has the capacity to classify regions to specific classes (person, car, chair, …etc.). It can also generate a <em class="lw">background</em> class, which causes the ROI to be discarded.</li><li id="77b5" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr ot np nq nr gb"><strong class="ky ji">Bounding Box Refinement:</strong> Very similar to how it’s done in the RPN, and its purpose is to further refine the location and size of the bounding box to encapsulate the object.</li></ol><blockquote class="lt lu lv"><p id="a79e" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>The classifier and bounding box regressor are created in <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L901" rel="noopener ugc nofollow" target="_blank">fpn_classifier_graph()</a>.</p></blockquote><h2 id="6611" class="oc mb jh bn mc od oe of mg og oh oi mk lf oj ok mm lj ol om mo ln on oo mq op gb">ROI Pooling</h2><p id="01b2" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">There is a bit of a problem to solve before we continue. Classifiers don’t handle variable input size very well. They typically require a fixed input size. But, due to the bounding box refinement step in the RPN, the ROI boxes can have different sizes. That’s where ROI Pooling comes into play.</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div class="gm gn ov"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1290/1*bsT00ickNk7vaRJNrTvKPQ.png" width="645" height="217" loading="lazy" role="presentation"/></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">The feature map here is from a low-level layer, for illustration, to make it easier to understand.</figcaption></figure><p id="cb1d" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">ROI pooling refers to cropping a part of a feature map and resizing it to a fixed size. It’s similar in principle to cropping part of an image and then resizing it (but there are differences in implementation details).</p><p id="3ba3" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">The authors of Mask R-CNN suggest a method they named ROIAlign, in which they sample the feature map at different points and apply a bilinear interpolation. In our implementation, we used TensorFlow’s <a class="au ls" href="https://www.tensorflow.org/api_docs/python/tf/image/crop_and_resize" rel="noopener ugc nofollow" target="_blank">crop_and_resize</a> function for simplicity and because it’s close enough for most purposes.</p><blockquote class="lt lu lv"><p id="5e88" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>ROI pooling is implemented in the class <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L344" rel="noopener ugc nofollow" target="_blank">PyramidROIAlign</a>.</p></blockquote><h1 id="814c" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">4. Segmentation Masks</h1><p id="0414" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">If you stop at the end of the last section then you have a <a class="au ls" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">Faster R-CNN</a> framework for object detection. The mask network is the addition that the Mask R-CNN paper introduced.</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div class="gm gn ow"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/910/1*l55WzUq1ZD2b5EGwW05LDA.png" width="455" height="201" loading="lazy" role="presentation"/></div></figure><p id="46db" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">The mask branch is a convolutional network that takes the positive regions selected by the ROI classifier and generates masks for them. The generated masks are low resolution: 28x28 pixels. But they are <em class="lw">soft</em> masks, represented by float numbers, so they hold more details than binary masks. The small mask size helps keep the mask branch light. During training, we scale down the ground-truth masks to 28x28 to compute the loss, and during inferencing we scale up the predicted masks to the size of the ROI bounding box and that gives us the final masks, one per object.</p><blockquote class="lt lu lv"><p id="63ed" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>The mask branch is in <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L957" rel="noopener ugc nofollow" target="_blank">build_fpn_mask_graph()</a>.</p></blockquote><h1 id="4f9e" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">Let’s Build a Color Splash Filter</h1><figure class="my mz na nb gy nc gm gn paragraph-image"><div class="gm gn ox"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/920/1*lAP6vX1tLQaxFn6XGEQ32g.gif" width="460" height="444" loading="lazy" role="presentation"/></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">Sample generated by this project</figcaption></figure><p id="e4db" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">Unlike most image editing apps that include this filter, our filter will be a bit smarter: It finds the objects automatically. Which becomes even more useful if you want to apply it to videos rather than a single image.</p><h1 id="89d9" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">Training Dataset</h1><p id="7371" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">Typically, I’d start by searching for public datasets that contain the objects I need. But in this case, I wanted to document the full cycle and show how to build a dataset from scratch.</p><p id="dfe0" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">I searched for balloon images on flickr, limiting the license type to “Commercial use &amp; mods allowed”. This returned more than enough images for my needs. I picked a total of 75 images and divided them into a training set and a validation set. Finding images is easy. Annotating them is the hard part.</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn oy"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*Q4tCdhwrklvJLM9zn5aDhg.png" width="700" height="541" loading="lazy" role="presentation"/></div></div></figure><p id="2c9d" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">Wait! Don’t we need, like, a million images to train a deep learning model? Sometimes you do, but often you don’t. I’m relying on two main points to reduce my training requirements significantly:</p><p id="879c" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">First, <em class="lw">transfer learning. </em>Which simply means that, instead of training a model from scratch, I start with a weights file that’s been trained on the COCO dataset (we provide that in the github repo). Although the COCO dataset does <strong class="ky ji">not</strong> contain a balloon class, it contains a lot of other images (~120K), so the trained weights have already learned a lot of the features common in natural images, which really helps. And, second, given the simple use case here, I’m not demanding high accuracy from this model, so the tiny dataset should suffice.</p><p id="416f" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">There are a lot of tools to annotate images. I ended up using <a class="au ls" href="http://www.robots.ox.ac.uk/~vgg/software/via/" rel="noopener ugc nofollow" target="_blank">VIA (VGG Image Annotator)</a> because of its simplicity. It’s a single HTML file that you download and open in a browser. Annotating the first few images was very slow, but once I got used to the user interface, I was annotating at around an object a minute.</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn oz"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*6SICkQA-YCLp88A7GFM4Ag.png" width="700" height="489" loading="lazy" role="presentation"/></div></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">UI of the VGG Image Annotator tool</figcaption></figure><p id="a07c" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">If you don’t like the VIA tool, here is a list of the other tools I tested:</p><ul class=""><li id="4e5e" class="nj nk jh ky b kz la lc ld lf nl lj nm ln nn lr no np nq nr gb"><a class="au ls" href="http://labelme2.csail.mit.edu/" rel="noopener ugc nofollow" target="_blank">LabelMe</a>: One of the most known tools. The UI was a bit too slow, though, especially when zooming in on large images.</li><li id="f464" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><a class="au ls" href="https://rectlabel.com/" rel="noopener ugc nofollow" target="_blank">RectLabel</a>: Simple and easy to work with. Mac only.</li><li id="7ae8" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><a class="au ls" href="https://www.labelbox.io/" rel="noopener ugc nofollow" target="_blank">LabelBox</a>: Pretty good for larger labeling projects and has options for different types of labeling tasks.</li><li id="2e31" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><a class="au ls" href="http://www.robots.ox.ac.uk/~vgg/software/via/" rel="noopener ugc nofollow" target="_blank">VGG Image Annotator (VIA)</a>: Fast, light, and really well designed. This is the one I ended up using.</li><li id="8ca5" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><a class="au ls" href="https://github.com/tylin/coco-ui" rel="noopener ugc nofollow" target="_blank">COCO UI</a>: The tool used to annotate the COCO dataset.</li></ul><h1 id="d280" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">Loading the Dataset</h1><p id="a153" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">There isn’t a universally accepted format to store segmentation masks. Some datasets save them as PNG images, others store them as polygon points, and so on. To handle all these cases, our implementation provides a Dataset class that you inherit from and then override a few functions to read your data in whichever format it happens to be.</p><p id="b712" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">The VIA tool saves the annotations in a JSON file, and each mask is a set of polygon points. I didn’t find documentation for the format, but it’s pretty easy to figure out by looking at the generated JSON. I included comments in the code to explain how the parsing is done.</p><blockquote class="lt lu lv"><p id="e128" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>An easy way to write code for a new dataset is to copy <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/coco.py" rel="noopener ugc nofollow" target="_blank">coco.py</a> and modify it to your needs. Which is what I did. I saved the new file as <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/balloon.py" rel="noopener ugc nofollow" target="_blank">balloons.py</a></p></blockquote><p id="2869" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">My <code class="fq pa pb pc pd b">BalloonDataset</code> class looks like this:</p><pre class="my mz na nb gy pe bt pf"><span id="71f6" class="gb oc mb jh pd b dn pg ph l pi">class <strong class="pd ji">BalloonDataset</strong>(utils.Dataset):</span><span id="1d9c" class="gb oc mb jh pd b dn pj pk pl pm pn ph l pi">    def <strong class="pd ji">load_balloons</strong>(self, dataset_dir, subset):<br/>        ...</span><span id="5c60" class="gb oc mb jh pd b dn pj pk pl pm pn ph l pi">    def <strong class="pd ji">load_mask</strong>(self, image_id):<br/>        ...</span><span id="a411" class="gb oc mb jh pd b dn pj pk pl pm pn ph l pi">    def <strong class="pd ji">image_reference</strong>(self, image_id):<br/>        ...</span></pre><p id="2f1b" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb"><code class="fq pa pb pc pd b">load_balloons</code> reads the JSON file, extracts the annotations, and iteratively calls the internal <code class="fq pa pb pc pd b">add_class</code> and <code class="fq pa pb pc pd b">add_image</code> functions to build the dataset.</p><blockquote class="lt lu lv"><p id="fd34" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><code class="fq pa pb pc pd b">load_mask</code> generates bitmap masks for every object in the image by drawing the polygons.</p></blockquote><p id="7af2" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb"><code class="fq pa pb pc pd b">image_reference</code> simply returns a string that identifies the image for debugging purposes. Here it simply returns the path of the image file.</p><p id="0866" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">You might have noticed that my class doesn’t contain functions to load images or return bounding boxes. The default <code class="fq pa pb pc pd b">load_image</code> function in the base <code class="fq pa pb pc pd b">Dataset</code> class handles loading images. And, bounding boxes are generated dynamically from the masks.</p><blockquote class="lt lu lv"><p id="dfeb" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>Your dataset might not be in JSON. My BalloonDataset class reads JSON because that’s what the VIA tool generates. Don’t convert your dataset to a format similar to COCO or the VIA format. Insetad, write your own Dataset class to load whichever format your dataset comes in. See the <a class="au ls" href="https://github.com/matterport/Mask_RCNN/tree/master/samples" rel="noopener ugc nofollow" target="_blank">samples</a> and notice how each uses its own Dataset class.</p></blockquote><h2 id="58a4" class="oc mb jh bn mc od oe of mg og oh oi mk lf oj ok mm lj ol om mo ln on oo mq op gb">Verify the Dataset</h2><p id="05e7" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">To verify that my new code is implemented correctly I added this <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/inspect_balloon_data.ipynb" rel="noopener ugc nofollow" target="_blank">Jupyter notebook</a>. It loads the dataset, visualizes masks and bounding boxes, and visualizes the anchors to verify that my anchor sizes are a good fit for my object sizes. Here is an example of what you should expect to see:</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn po"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*OKE6wyZFfh2f_aZ3rd9BRw.png" width="700" height="492" loading="lazy" role="presentation"/></div></div><figcaption class="ny bm go gm gn nz oa bn b bo bp co">Sample from inspect_balloon_data notebook</figcaption></figure><blockquote class="lt lu lv"><p id="db09" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>To create this notebook I copied <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/inspect_data.ipynb" rel="noopener ugc nofollow" target="_blank">inspect_data.ipynb</a>, which we wrote for the COCO dataset, and modified one block of code at the top to load the Balloons dataset instead.</p></blockquote><h1 id="da5a" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">Configurations</h1><p id="e824" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">The configurations for this project are similar to the base configuration used to train the COCO dataset, so I just needed to override 3 values. As I did with the <code class="fq pa pb pc pd b">Dataset</code> class, I inherit from the base <code class="fq pa pb pc pd b">Config</code> class and add my overrides:</p><pre class="my mz na nb gy pe bt pf"><span id="b1b8" class="gb oc mb jh pd b dn pg ph l pi">class BalloonConfig(Config):</span><span id="df09" class="gb oc mb jh pd b dn pj pk pl pm pn ph l pi">    # Give the configuration a recognizable name<br/>    NAME = &quot;balloons&quot;</span><span id="569d" class="gb oc mb jh pd b dn pj pk pl pm pn ph l pi">    # Number of classes (including background)<br/>    NUM_CLASSES = 1 + 1  # Background + balloon</span><span id="db8c" class="gb oc mb jh pd b dn pj pk pl pm pn ph l pi">    # Number of training steps per epoch<br/>    STEPS_PER_EPOCH = 100</span></pre><p id="004b" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">The base configuration uses input images of size 1024x1024 px for best accuracy. I kept it that way. My images are a bit smaller, but the model resizes them automatically.</p><blockquote class="lt lu lv"><p id="b92d" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>The base Config class is in <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/config.py" rel="noopener ugc nofollow" target="_blank">config.py</a>. And BalloonConfig is in<a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/balloon.py#L61" rel="noopener ugc nofollow" target="_blank"> balloons.py</a>.</p></blockquote><h1 id="1f2b" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">Training</h1><p id="0a7c" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">Mask R-CNN is a fairly large model. Especially that our implementation uses ResNet101 and FPN. So you need a modern GPU with 12GB of memory. It might work on less, but I haven’t tried. I used <a class="au ls" href="https://aws.amazon.com/ec2/instance-types/p2/" rel="noopener ugc nofollow" target="_blank">Amazon’s P2 instances</a> to train this model, and given the small dataset, training takes less than an hour.</p><p id="7837" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">Start the training with this command, running from the <code class="fq pa pb pc pd b">balloon</code> directory. Here, we’re specifying that training should start from the pre-trained COCO weights. The code will download the weights from our repository automatically:</p><pre class="my mz na nb gy pe bt pf"><span id="c106" class="gb oc mb jh pd b dn pg ph l pi">python3 balloon.py train --dataset=/path/to/dataset <strong class="pd ji">--model=coco</strong></span></pre><p id="fc26" class="pw-post-body-paragraph kw kx jh ky b kz la ki lb lc ld kl le lf lg lh li lj lk ll lm ln lo lp lq lr ja gb">And to resume training if it stopped:</p><pre class="my mz na nb gy pe bt pf"><span id="b555" class="gb oc mb jh pd b dn pg ph l pi">python3 balloon.py train --dataset=/path/to/dataset <strong class="pd ji">--model=last</strong></span></pre><blockquote class="lt lu lv"><p id="d71e" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:<br/></strong>In addition to balloons.py, the repository has three more examples: <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb" rel="noopener ugc nofollow" target="_blank">train_shapes.ipynb</a> which trains a toy model to detect geometric shapes, <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/coco.py" rel="noopener ugc nofollow" target="_blank">coco.py</a> which trains on the COCO dataset, and <a class="au ls" href="https://github.com/matterport/Mask_RCNN/tree/master/samples/nucleus" rel="noopener ugc nofollow" target="_blank">nucleus</a> which segments nuclei in microscopy images.</p></blockquote><h1 id="6bcb" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">Inspecting the Results</h1><p id="a77c" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">The <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/inspect_balloon_model.ipynb" rel="noopener ugc nofollow" target="_blank">inspect_balloon_model</a> notebook shows the results generated by the trained model. Check the notebook for more visualizations and a step by step walk through the detection pipeline.</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn pp"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*BvqnziHW514YyO20UNtS3g.png" width="700" height="300" loading="lazy" role="presentation"/></div></div></figure><blockquote class="lt lu lv"><p id="29f6" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>This notebook is a simplified version of <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/master/samples/coco/inspect_model.ipynb" rel="noopener ugc nofollow" target="_blank">inspect_mode.ipynb</a>, which includes visualizations and debugging code for the COCO dataset.</p></blockquote><h1 id="1b36" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">Color Splash</h1><p id="5e81" class="pw-post-body-paragraph kw kx jh ky b kz ms ki lb lc mt kl le lf mu lh li lj mv ll lm ln mw lp lq lr ja gb">Finally, now that we have object masks, let’s use them to apply the color splash effect. The method is really simple: create a grayscale version of the image, and then, in areas marked by the object mask, copy back the color pixels from original image. Here is an example:</p><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn pq"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*iPAtWFnShPhX5atbY3V0pQ.png" width="700" height="368" loading="lazy" role="presentation"/></div></div></figure><blockquote class="lt lu lv"><p id="5917" class="kw kx lw ky b kz la ki lb lc ld kl le lx lg lh li ly lk ll lm lz lo lp lq lr ja gb"><strong class="ky ji">Code Tip:</strong><br/>The code that applies the effect is in the <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/balloon.py#L201" rel="noopener ugc nofollow" target="_blank">color_splash()</a> function. And <a class="au ls" href="https://github.com/matterport/Mask_RCNN/blob/v2.1/samples/balloon/balloon.py#L221" rel="noopener ugc nofollow" target="_blank">detect_and_color_splash()</a> handles the whole process from loading the image, running instance segmentation, and applying the color splash filter.</p></blockquote><h1 id="999f" class="ma mb jh bn mc md me mf mg mh mi mj mk kn ml ko mm kq mn kr mo kt mp ku mq mr gb">FAQ</h1><ul class=""><li id="3c51" class="nj nk jh ky b kz ms lc mt lf pr lj ps ln pt lr no np nq nr gb"><strong class="ky ji">Q:</strong> I want to dive deeper and understand the details, what should I read?<br/><strong class="ky ji">A:</strong> Read these papers in this order: <a class="au ls" href="http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=AF8817DD0F70B32AA08B2ECBBA8099FA?doi=10.1.1.715.2453&amp;rep=rep1&amp;type=pdf" rel="noopener ugc nofollow" target="_blank">RCNN (pdf)</a>, <a class="au ls" href="https://arxiv.org/abs/1504.08083" rel="noopener ugc nofollow" target="_blank">Fast RCNN</a>, <a class="au ls" href="https://arxiv.org/abs/1506.01497" rel="noopener ugc nofollow" target="_blank">Faster RCNN</a>, <a class="au ls" href="https://arxiv.org/abs/1612.03144" rel="noopener ugc nofollow" target="_blank">FPN</a>, <a class="au ls" href="https://arxiv.org/abs/1703.06870" rel="noopener ugc nofollow" target="_blank">Mask RCNN</a>.</li><li id="7e3a" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><strong class="ky ji">Q:</strong> Where can I ask more questions?<br/><strong class="ky ji">A:</strong> The <a class="au ls" href="https://github.com/matterport/Mask_RCNN/issues" rel="noopener ugc nofollow" target="_blank">Issues page on GitHub</a> is active, you can use it for questions, as well as to report issues. Remember to search closed issues as well in case your question has been answered already.</li><li id="fb26" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><strong class="ky ji">Q:</strong> Can I contribute to this project?<br/><strong class="ky ji">A:</strong> That would be great. Pull Requests are always welcome.</li><li id="dec0" class="nj nk jh ky b kz ns lc nt lf nu lj nv ln nw lr no np nq nr gb"><strong class="ky ji">Q:</strong> Can I join your team and work on fun projects like this one?<br/><strong class="ky ji">A:</strong> Yes, we’re hiring for deep learning and computer vision. <a class="au ls" href="https://matterport.com/careers/" rel="noopener ugc nofollow" target="_blank">Apply here</a>.</li></ul><figure class="my mz na nb gy nc gm gn paragraph-image"><div role="button" tabindex="0" class="nd ne dp nf cf ng"><div class="gm gn pu"><img alt="" class="cf nh ni" src="https://miro.medium.com/max/1400/1*w_ownWZZ38QhiVjVU757DA.png" width="700" height="464" loading="lazy" role="presentation"/></div></div></figure></div></div></section></div></div></article><div class="pv o"></div></div></div><div class="l"></div><footer class="pw px py pz o ao qa qb c"><div class="l qc"><div class="o dy"><div class="en cf fb fc fd fe ff fg fh fi fj"><div class="o u qd"><div class="o ao hd"><div class="qe l"><span class="l hf qf qg e d"><div class="o ao hd"><div class="pw-multi-vote-icon dp qh qi qj qk"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmatterport-engineering%2F7c761e238b46&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;user=Waleed+Abdulla&amp;userId=1a69ae209bc4&amp;source=-----7c761e238b46---------------------clap_footer-----------" rel="noopener follow"><div class="ds ie df ql qm qn bb qo qp qq qk"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></a></span></div><div class="pw-multi-vote-count l qr qs qt qu qv qw qx"><p class="bn b hj bp co"><span class="qy">--</span></p></div></div></span><span class="l h g f qz ra"><div class="o ao hd"><div class="pw-multi-vote-icon dp qh qi qj qk"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fvote%2Fmatterport-engineering%2F7c761e238b46&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;user=Waleed+Abdulla&amp;userId=1a69ae209bc4&amp;source=-----7c761e238b46---------------------clap_footer-----------" rel="noopener follow"><div class="ds ie df ql qm qn bb qo qp qq qk"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="clap"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.37.83L12 3.28l.63-2.45h-1.26zM13.92 3.95l1.52-2.1-1.18-.4-.34 2.5zM8.59 1.84l1.52 2.11-.34-2.5-1.18.4zM18.52 18.92a4.23 4.23 0 0 1-2.62 1.33l.41-.37c2.39-2.4 2.86-4.95 1.4-7.63l-.91-1.6-.8-1.67c-.25-.56-.19-.98.21-1.29a.7.7 0 0 1 .55-.13c.28.05.54.23.72.5l2.37 4.16c.97 1.62 1.14 4.23-1.33 6.7zm-11-.44l-4.15-4.15a.83.83 0 0 1 1.17-1.17l2.16 2.16a.37.37 0 0 0 .51-.52l-2.15-2.16L3.6 11.2a.83.83 0 0 1 1.17-1.17l3.43 3.44a.36.36 0 0 0 .52 0 .36.36 0 0 0 0-.52L5.29 9.51l-.97-.97a.83.83 0 0 1 0-1.16.84.84 0 0 1 1.17 0l.97.97 3.44 3.43a.36.36 0 0 0 .51 0 .37.37 0 0 0 0-.52L6.98 7.83a.82.82 0 0 1-.18-.9.82.82 0 0 1 .76-.51c.22 0 .43.09.58.24l5.8 5.79a.37.37 0 0 0 .58-.42L13.4 9.67c-.26-.56-.2-.98.2-1.29a.7.7 0 0 1 .55-.13c.28.05.55.23.73.5l2.2 3.86c1.3 2.38.87 4.59-1.29 6.75a4.65 4.65 0 0 1-4.19 1.37 7.73 7.73 0 0 1-4.07-2.25zm3.23-12.5l2.12 2.11c-.41.5-.47 1.17-.13 1.9l.22.46-3.52-3.53a.81.81 0 0 1-.1-.36c0-.23.09-.43.24-.59a.85.85 0 0 1 1.17 0zm7.36 1.7a1.86 1.86 0 0 0-1.23-.84 1.44 1.44 0 0 0-1.12.27c-.3.24-.5.55-.58.89-.25-.25-.57-.4-.91-.47-.28-.04-.56 0-.82.1l-2.18-2.18a1.56 1.56 0 0 0-2.2 0c-.2.2-.33.44-.4.7a1.56 1.56 0 0 0-2.63.75 1.6 1.6 0 0 0-2.23-.04 1.56 1.56 0 0 0 0 2.2c-.24.1-.5.24-.72.45a1.56 1.56 0 0 0 0 2.2l.52.52a1.56 1.56 0 0 0-.75 2.61L7 19a8.46 8.46 0 0 0 4.48 2.45 5.18 5.18 0 0 0 3.36-.5 4.89 4.89 0 0 0 4.2-1.51c2.75-2.77 2.54-5.74 1.43-7.59L18.1 7.68z"></path></svg></div></a></span></div><div class="pw-multi-vote-count l qr qs qt qu qv qw qx"><p class="bn b hj bp co"><span class="qy">--</span></p></div></div></span></div><div class="rb o"><div><div class="cj" aria-hidden="false"><button class="ie ds rd o ao df qm re" aria-label="responses"><svg width="24" height="24" viewBox="0 0 24 24" aria-label="responses" class="rc"><path d="M18 16.8a7.14 7.14 0 0 0 2.24-5.32c0-4.12-3.53-7.48-8.05-7.48C7.67 4 4 7.36 4 11.48c0 4.13 3.67 7.48 8.2 7.48a8.9 8.9 0 0 0 2.38-.32c.23.2.48.39.75.56 1.06.69 2.2 1.04 3.4 1.04.22 0 .4-.11.48-.29a.5.5 0 0 0-.04-.52 6.4 6.4 0 0 1-1.16-2.65v.02zm-3.12 1.06l-.06-.22-.32.1a8 8 0 0 1-2.3.33c-4.03 0-7.3-2.96-7.3-6.59S8.17 4.9 12.2 4.9c4 0 7.1 2.96 7.1 6.6 0 1.8-.6 3.47-2.02 4.72l-.2.16v.26l.02.3a6.74 6.74 0 0 0 .88 2.4 5.27 5.27 0 0 1-2.17-.86c-.28-.17-.72-.38-.94-.59l.01-.02z"></path></svg><p class="bn b bo bp co"><span class="pw-responses-count rf rc rg">69</span></p></button></div></div></div></div><div class="o ao"><div class="cj" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="cj" aria-hidden="false"><button class="au av aw ax ay az ba ia bc bd be bf bg bh bi rh ib ri" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="24" height="24" viewBox="0 0 24 24" fill="none"><path fill-rule="evenodd" clip-rule="evenodd" d="M15.22 4.93a.42.42 0 0 1-.12.13h.01a.45.45 0 0 1-.29.08.52.52 0 0 1-.3-.13L12.5 3v7.07a.5.5 0 0 1-.5.5.5.5 0 0 1-.5-.5V3.02l-2 2a.45.45 0 0 1-.57.04h-.02a.4.4 0 0 1-.16-.3.4.4 0 0 1 .1-.32l2.8-2.8a.5.5 0 0 1 .7 0l2.8 2.8a.42.42 0 0 1 .07.5zm-.1.14zm.88 2h1.5a2 2 0 0 1 2 2v10a2 2 0 0 1-2 2h-11a2 2 0 0 1-2-2v-10a2 2 0 0 1 2-2H8a.5.5 0 0 1 .35.14c.1.1.15.22.15.35a.5.5 0 0 1-.15.35.5.5 0 0 1-.35.15H6.4c-.5 0-.9.4-.9.9v10.2a.9.9 0 0 0 .9.9h11.2c.5 0 .9-.4.9-.9V8.96c0-.5-.4-.9-.9-.9H16a.5.5 0 0 1 0-1z" fill="#000"></path></svg></button></div></div></div><div class="rj l fs"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fbookmark%2Fp%2F7c761e238b46&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;source=--------------------------bookmark_footer-----------" rel="noopener follow"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="au df aw ax ay az ba ia bc ie if ig ih"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="id" aria-label="Add to list bookmark button"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></a></span></div></div></div></div></div></div></footer></div><div class="o dy"><div class="en cf fb fc fd fe ff fg fh fi fj"></div></div><div class="l"><div class="l rk qd"><div class="l qd"><div class="rl rm l rk"><div class="o dy"><div class="en cf fb fc fd fe ff fg fh fi fj"><div class="o ao u"><h2 class="bn rn od of ro mg og oi rp mk lf ok rq mm lj om rr mo ln oo rs mq ft fv fw fx fy fz ga gb"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://engineering.matterport.com/?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow">More from Matterport Engineering Techblog</a></h2><span><a class="bn b bo bp hk br hm hn ho hp hq bd ca hr hs ht ce cg ch ci cj ck" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fcollection%2Fmatterport-engineering%2F7c761e238b46&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;collection=Matterport+Engineering+Techblog&amp;collectionId=e3a700e43a0d&amp;source=post_page-----7c761e238b46---------------------follow_footer-----------" rel="noopener follow">Follow</a></span></div><div class="rt l"><p class="bn b bo bp co">Thoughts on 3D from Matterport’s web and vision teams.</p></div></div></div></div></div><div class="ru l"><div class="rv rw l"><div class="nh l bm"><a class="bn b bo bp hk br hm hn ho hp hq bd ca hr hs ht ce cg ch ci cj ck" href="https://engineering.matterport.com/?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow">Read more from <!-- -->Matterport Engineering Techblog</a></div></div></div><div class="o dy"><div class="en cf fb fc fd fe ff fg fh fi fj"><div class="rx rw l"><section class="pw-more-medium-articles l"><div class="ry l"><h2 class="bn rn od of ro mg og oi rp mk lf ok rq mm lj om rr mo ln oo rs mq ft fv fw fx fy fz ga gb">Recommended from Medium</h2></div><div class="gz o hd hu rz sa sb sc sd se sf sg sh si sj sk sl sm sn"><div class="so sp sq sr ss st su sv sw sx sy sz ta tb tc td te tf tg th ti"><div class="cf ag"><div class="tj l"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://aniltilbe.medium.com/?source=post_internal_links---------0----------------------------" rel="noopener follow"><div class="l dp"><img alt="Anil Tilbe" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/1*7teQ7WGLK11vff4j8_rZ1Q.png" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://aniltilbe.medium.com/?source=post_internal_links---------0----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Anil Tilbe</p></a></div></div></div></div><div class="hz l"><p class="bn b hj bp co">in</p></div><div class="l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://pub.towardsai.net/?source=post_internal_links---------0----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Towards AI</p></a></div></div><a href="https://pub.towardsai.net/zero-shot-learning-deep-dive-how-to-select-one-and-challenges-60ae243e040a?source=post_internal_links---------0----------------------------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Zero-shot Learning Deep Dive: How to Select One and Present-day Challenges</div></h2></a></div><a href="https://pub.towardsai.net/zero-shot-learning-deep-dive-how-to-select-one-and-challenges-60ae243e040a?source=post_internal_links---------0----------------------------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="A dart board" class="tw" src="https://miro.medium.com/focal/112/112/50/50/1*AKTFhzlZzW5aM_VnkiD89g.jpeg" width="56"/></div></div></div></a></div></div></div></div></div><div class="so sp sq sr ss st su sv sw sx sy sz ta tb tc td te tf tg th ti"><div class="cf ag"><div class="tj l"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@luckyabolorunke?source=post_internal_links---------1----------------------------" rel="noopener follow"><div class="l dp"><img alt="Lucky Abolorunke" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/1*Y7NrYsAlCR_wcjucCYSGRw.jpeg" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@luckyabolorunke?source=post_internal_links---------1----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Lucky Abolorunke</p></a></div></div></div></div></div><a href="https://medium.com/@luckyabolorunke/how-to-build-a-simple-pipeline-in-machine-learning-d9819ca49d1c?source=post_internal_links---------1----------------------------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>How To Build A Simple Pipeline In Machine Learning</div></h2></a></div><a href="https://medium.com/@luckyabolorunke/how-to-build-a-simple-pipeline-in-machine-learning-d9819ca49d1c?source=post_internal_links---------1----------------------------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/1*BMnXJ6R0uukYxeN1oZnu8g.jpeg" width="56" role="presentation"/></div></div></div></a></div></div></div></div></div><div class="so sp sq sr ss st su sv sw sx sy sz ta tb tc td te tf tg th ti"><div class="cf ag"><div class="tj l"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://saswatac.medium.com/?source=post_internal_links---------2----------------------------" rel="noopener follow"><div class="l dp"><img alt="Saswata Chakravarty" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/2*RxNpCf2tV662W5gz7xEQ8g.jpeg" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://saswatac.medium.com/?source=post_internal_links---------2----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Saswata Chakravarty</p></a></div></div></div></div></div><a href="https://saswatac.medium.com/speeding-up-data-pipelines-for-deep-learning-using-ray-6b043212274d?source=post_internal_links---------2----------------------------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Speeding up data pipelines for deep learning using Ray</div></h2></a></div><a href="https://saswatac.medium.com/speeding-up-data-pipelines-for-deep-learning-using-ray-6b043212274d?source=post_internal_links---------2----------------------------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/0*VInTLbxKhnOR5HD9" width="56" role="presentation"/></div></div></div></a></div></div></div></div></div><div class="so sp sq sr ss st su sv sw sx sy sz ta tb tc td te tf tg th ti"><div class="cf ag"><div class="tj l"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@manujosephv?source=post_internal_links---------3----------------------------" rel="noopener follow"><div class="l dp"><img alt="Manu Joseph" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/2*eVvYVSkqyFwiuwJGfpCduw.png" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@manujosephv?source=post_internal_links---------3----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Manu Joseph</p></a></div></div></div></div><div class="hz l"><p class="bn b hj bp co">in</p></div><div class="l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://towardsdatascience.com/?source=post_internal_links---------3----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Towards Data Science</p></a></div></div><a href="https://towardsdatascience.com/forecast-error-measures-scaled-relative-and-other-errors-c0645f794352?source=post_internal_links---------3----------------------------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Forecast Error Measures: Scaled, Relative, and other Errors</div></h2></a></div><a href="https://towardsdatascience.com/forecast-error-measures-scaled-relative-and-other-errors-c0645f794352?source=post_internal_links---------3----------------------------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/0*i65JPk7qXqBIlh4t" width="56" role="presentation"/></div></div></div></a></div></div></div></div></div><div class="so sp sq sr ss st su sv sw sx sy sz ta tb tc td te tf tg th ti"><div class="cf ag"><div class="tj l"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://deepankar20007.medium.com/?source=post_internal_links---------4----------------------------" rel="noopener follow"><div class="l dp"><img alt="Deepankar" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/1*efTzcp5X1xcM2Y48zhRnCQ.png" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://deepankar20007.medium.com/?source=post_internal_links---------4----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Deepankar</p></a></div></div></div></div><div class="hz l"><p class="bn b hj bp co">in</p></div><div class="l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/analytics-vidhya?source=post_internal_links---------4----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Analytics Vidhya</p></a></div></div><a href="https://medium.com/analytics-vidhya/human-face-emotion-recognition-system-460436ee2393?source=post_internal_links---------4----------------------------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Human Face Emotion Recognition System</div></h2></a></div><a href="https://medium.com/analytics-vidhya/human-face-emotion-recognition-system-460436ee2393?source=post_internal_links---------4----------------------------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/1*b53T_QLDJEekK5XnKJBSbA.png" width="56" role="presentation"/></div></div></div></a></div></div></div></div></div><div class="so sp sq sr ss st su sv sw sx sy sz ta tb tc td te tf tg th ti"><div class="cf ag"><div class="tj l"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@pedroecarrionz?source=post_internal_links---------5----------------------------" rel="noopener follow"><div class="l dp"><img alt="Pedro Carrión" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/1*jcbLQiZjnyiJwKXH6lizLw.png" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@pedroecarrionz?source=post_internal_links---------5----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Pedro Carrión</p></a></div></div></div></div></div><a href="https://medium.com/@pedroecarrionz/basics-of-clustering-450160f3b43b?source=post_internal_links---------5----------------------------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Basics of Clustering</div></h2></a></div><a href="https://medium.com/@pedroecarrionz/basics-of-clustering-450160f3b43b?source=post_internal_links---------5----------------------------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/freeze/focal/112/112/50/50/0*20Hl5lwM9WWpzjyX.gif" width="56" role="presentation"/></div></div></div></a></div></div></div></div></div><div class="so sp sq sr ss st su sv sw sx sy sz ta tb tc td te tf tg th ti"><div class="cf ag"><div class="tj l"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@nawazahmad20?source=post_internal_links---------6----------------------------" rel="noopener follow"><div class="l dp"><img alt="nawaz ahmad" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/2*8im3QUtZ-D-F6Fi0nGmP5w.jpeg" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@nawazahmad20?source=post_internal_links---------6----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">nawaz ahmad</p></a></div></div></div></div><div class="hz l"><p class="bn b hj bp co">in</p></div><div class="l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://towardsdatascience.com/?source=post_internal_links---------6----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Towards Data Science</p></a></div></div><a href="https://towardsdatascience.com/line-follower-robot-using-cnn-4bb4f297c672?source=post_internal_links---------6----------------------------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Line Follower Robot using CNN</div></h2></a></div><a href="https://towardsdatascience.com/line-follower-robot-using-cnn-4bb4f297c672?source=post_internal_links---------6----------------------------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/1*opeFd5syg5Rd3M6MrHjV1g.jpeg" width="56" role="presentation"/></div></div></div></a></div></div></div></div></div><div class="so sp sq sr ss st su sv sw sx sy sz ta tb tc td te tf tg th ti"><div class="cf ag"><div class="tj l"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@ardito.bryan?source=post_internal_links---------7----------------------------" rel="noopener follow"><div class="l dp"><img alt="Michelangiolo Mazzeschi" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/2*MkUxrUogzkaAyb_Nf76wRQ.jpeg" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@ardito.bryan?source=post_internal_links---------7----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Michelangiolo Mazzeschi</p></a></div></div></div></div><div class="hz l"><p class="bn b hj bp co">in</p></div><div class="l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://pub.towardsai.net/?source=post_internal_links---------7----------------------------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Towards AI</p></a></div></div><a href="https://pub.towardsai.net/wine-classifier-using-supervised-learning-with-98-accuracy-5f2e173e967e?source=post_internal_links---------7----------------------------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Wine Classifier Using Supervised Learning with 98% Accuracy</div></h2></a></div><a href="https://pub.towardsai.net/wine-classifier-using-supervised-learning-with-98-accuracy-5f2e173e967e?source=post_internal_links---------7----------------------------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/0*K6IV6ZnHZ0nmq_M6" width="56" role="presentation"/></div></div></div></a></div></div></div></div></div></div></section></div></div></div></div></div><div class="d"><div class="tx ty tz l qa ar ee as ua ub"><div class="o dy"><div class="fb fc uc ud ue uf en cf"><a class="au av aw ax ay az ba bb bc bd ug uh bg ui uj" aria-label="Go to homepage" href="https://medium.com/?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow"><svg viewBox="0 0 3940 610" class="hm uk"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><div class="ul l"><p class="bn b hj bp um"><a class="au av aw ax ay az ba bb bc bd un bg ui uj uo" href="https://medium.com/about?autoplay=1&amp;source=post_page-----7c761e238b46--------------------------------" rel="noopener follow">About</a><a class="au av aw ax ay az ba bb bc bd un bg ui uj uo" href="https://help.medium.com/hc/en-us?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow">Help</a><a class="au av aw ax ay az ba bb bc bd un bg ui uj uo" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow">Terms</a><a class="au av aw ax ay az ba bb bc bd un bg ui uj" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----7c761e238b46--------------------------------" rel="noopener follow">Privacy</a></p></div><div class="j i d"><hr class="ds dt up uq" aria-hidden="true"/><h2 class="bn rn dn bp jg um">Get the Medium app</h2><div class="ul o"><div class="fk l"><a class="au av aw ax ay az ba bb bc bd ug uh bg ui uj" href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----7c761e238b46--------------------------------" rel="noopener follow"><img alt="A button that says &#x27;Download on the App Store&#x27;, and if clicked it will lead you to the iOS App store" class="" src="https://miro.medium.com/max/270/1*Crl55Tm6yDNMoucPo1tvDg.png" width="135" height="41"/></a></div><a class="au av aw ax ay az ba bb bc bd ug uh bg ui uj" href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----7c761e238b46--------------------------------" rel="noopener follow"><img alt="A button that says &#x27;Get it on, Google Play&#x27;, and if clicked it will lead you to the Google Play store" class="" src="https://miro.medium.com/max/270/1*W_RAPQ62h0em559zluJLdQ.png" width="135" height="41"/></a></div></div></div></div></div></div></div></main><div class="ep ci c eq h k j i cv er es et"><div class="ag cf cj dp"><div class="l dc aq"><div class="er o db"><div class="l qc"><div class="l c"><div class="l"><div class="rw o ao"><div class="pw-susi-button l qc"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;source=post_page--------------------------nav_reg-----------" rel="noopener follow"><button class="bn b bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd ce cf cg ch ci cj ck" aria-label="sign up">Get started</button></a></span></div></div></div><div class="ur l"><div class="o im us ut"><div class="cj" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"></div><span class="uy l"><svg width="25" height="25" viewBox="0 0 25 25" fill="rgba(8, 8, 8, 1)"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></span><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-label="search" tabindex="0" class="ea uu bn bo bp uo cf uv uw gb ux" placeholder="Search" value=""/></div><div class="uz l"></div><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@waleedka" rel="noopener follow"><div class="l dp"><img alt="Waleed Abdulla" class="l ci fm va vb fq" src="https://miro.medium.com/fit/c/176/176/0*RDTSqB0ocRKRgYEA.JPG" width="88" height="88"/><div class="fl fm l va vb fp aq"></div></div></a><div class="vc l"></div><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@waleedka" rel="noopener follow"><h2 class="pw-author-name bn rn dn bp jg gb">Waleed Abdulla</h2></a><div class="vd l"></div><span class="pw-follower-count bn b dn do co"><button class="au av aw ax ay az ba bb bc bd be bf bg bh bi">1.6K Followers</button></span><div class="ve l"></div><p class="bn b bo bp co">Startups, deep learning, computer vision.</p><div class="vf l"></div><div class="vg o"><span><a class="bn b bo bp hk br hm hn ho hp hq bd ca hr hs ht ce vh cg ch ci cj ck" href="https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F1a69ae209bc4&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;user=Waleed+Abdulla&amp;userId=1a69ae209bc4&amp;source=post_page-1a69ae209bc4-------------------------follow_profile-----------" rel="noopener follow">Follow</a></span><div class="vi l"><div><div><div class="cj" aria-hidden="false"><div class="l"><span><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/m/signin?actionUrl=%2F_%2Fapi%2Fsubscriptions%2Fnewsletters%2F1f594181b9d7&amp;operation=register&amp;redirect=https%3A%2F%2Fengineering.matterport.com%2Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46&amp;newsletterV3=1a69ae209bc4&amp;newsletterV3Id=1f594181b9d7&amp;user=Waleed+Abdulla&amp;userId=1a69ae209bc4&amp;source=--------------------------subscribe_user-----------" rel="noopener follow"><button class="bn b bo bp bq bb bs bt bu bv bw bx by bz ca hr hs ht ce cg ch ci cj ck" aria-label="Subscribe"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="vj vk vl"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5"></path><path d="M11.5 14.5L19 20l4-3"></path></svg></button></a></span></div></div></div></div></div></div><div class="vm l"><div class="l"><div class="tk uz l"><h2 class="bn rn dn bp jg gb">More from <!-- -->Medium</h2></div><div class="l"><div class="cf ag"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@bubblegum.frivolous?source=read_next_recirc---------0---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><div class="l dp"><img alt="Sneha Nanavati" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/1*i4XQzztpUqzvEoEtVvkQOA.jpeg" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@bubblegum.frivolous?source=read_next_recirc---------0---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Sneha Nanavati</p></a></div></div></div></div><div class="hz l"><p class="bn b hj bp co">in</p></div><div class="l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://blog.aicrowd.com/?source=read_next_recirc---------0---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">AIcrowd</p></a></div></div><a href="https://blog.aicrowd.com/faces-ai-blitz-xiii-with-team-glados-a00b54245450?source=read_next_recirc---------0---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Faces: AI Blitz XIII with Team GLaDOS</div></h2></a></div><a href="https://blog.aicrowd.com/faces-ai-blitz-xiii-with-team-glados-a00b54245450?source=read_next_recirc---------0---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/1*kwuOdYjO5Xmyi9dm5JUhmw.png" width="56" role="presentation"/></div></div></div></a></div></div></div><div class="cf ag"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://augmentedstartups.medium.com/?source=read_next_recirc---------1---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><div class="l dp"><img alt="Ritesh Kanjee" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/1*NCewNVNQJ2XPEWaS3LcO8g.jpeg" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://augmentedstartups.medium.com/?source=read_next_recirc---------1---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Ritesh Kanjee</p></a></div></div></div></div><div class="hz l"><p class="bn b hj bp co">in</p></div><div class="l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/augmented-startups?source=read_next_recirc---------1---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Augmented Startups</p></a></div></div><a href="https://medium.com/augmented-startups/mean-average-precision-map-fun-easy-explanation-978026bbcdfb?source=read_next_recirc---------1---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Mean Average Precision (mAP) — Fun &amp; Easy Explanation</div></h2></a></div><a href="https://medium.com/augmented-startups/mean-average-precision-map-fun-easy-explanation-978026bbcdfb?source=read_next_recirc---------1---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/1*HgAslEcX1QP1Nx4wZe1lkA.png" width="56" role="presentation"/></div></div></div></a></div></div></div><div class="cf ag"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@BH_Chinmay?source=read_next_recirc---------2---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><div class="l dp"><img alt="Chinmay Bhalerao" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/1*WuMKKoy4qWUenOofNA-Q3g.jpeg" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@BH_Chinmay?source=read_next_recirc---------2---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Chinmay Bhalerao</p></a></div></div></div></div></div><a href="https://medium.com/@BH_Chinmay/calibration-in-image-processing-c4c164870f21?source=read_next_recirc---------2---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Calibration in Image Processing</div></h2></a></div><a href="https://medium.com/@BH_Chinmay/calibration-in-image-processing-c4c164870f21?source=read_next_recirc---------2---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/0*zf-wVE5ZdH3v4IHB" width="56" role="presentation"/></div></div></div></a></div></div></div><div class="cf ag"><div class="tk o db dy"><div class="o hd u"><div class="tl o db tm"><div class="tn o ao"><div class="to l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@vaibhavbagri07?source=read_next_recirc---------3---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><div class="l dp"><img alt="Vaibhav Bagri" class="l ci fm tp tq fq" src="https://miro.medium.com/fit/c/40/40/0*aj14zg1HdhVibwC_" width="20" height="20"/><div class="fl fm l tp tq fp aq"></div></div></a></div><div class="hz l"><div><div class="cj" aria-hidden="false"><div class="o"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/@vaibhavbagri07?source=read_next_recirc---------3---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><p class="bn b hj bp ft fu fv fw fx fy fz ga gb">Vaibhav Bagri</p></a></div></div></div></div></div><a href="https://medium.com/@vaibhavbagri07/looking-at-research-work-in-real-time-object-detection-a6a642271051?source=read_next_recirc---------3---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><h2 class="bn ji dn bp ft tr fv fw ts fy ga jg gb"><div>Looking at Research Work in Real Time Object Detection</div></h2></a></div><a href="https://medium.com/@vaibhavbagri07/looking-at-research-work-in-real-time-object-detection-a6a642271051?source=read_next_recirc---------3---------------------6c28a0c7_8749_4c40_90db_d09ae5242f51-------" rel="noopener follow"><div class="tt l"><div class="m l dp fq"><div class="tu tv l"><img alt="" class="tw" src="https://miro.medium.com/focal/112/112/50/50/0*TpwrVjUKUWL9D36r.jpg" width="56" role="presentation"/></div></div></div></a></div></div></div></div></div></div></div></div></div><div class="vn o hd hu"><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://help.medium.com/hc/en-us" rel="noopener follow"><p class="bn b vp vq co">Help</p></a></div><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.statuspage.io" rel="noopener follow"><p class="bn b vp vq co">Status</p></a></div><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://about.medium.com/creators/" rel="noopener follow"><p class="bn b vp vq co">Writers</p></a></div><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://blog.medium.com" rel="noopener follow"><p class="bn b vp vq co">Blog</p></a></div><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e" rel="noopener follow"><p class="bn b vp vq co">Careers</p></a></div><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://policy.medium.com/medium-privacy-policy-f03bf92035c9" rel="noopener follow"><p class="bn b vp vq co">Privacy</p></a></div><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f" rel="noopener follow"><p class="bn b vp vq co">Terms</p></a></div><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://medium.com/about?autoplay=1" rel="noopener follow"><p class="bn b vp vq co">About</p></a></div><div class="vo l"><a class="au av aw ax ay az ba bb bc bd be bf bg bh bi" href="https://knowable.fyi" rel="noopener follow"><p class="bn b vp vq co">Knowable</p></a></div></div></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20220825-212037-fc552aa4bb"</script><script>window.__GRAPHQL_URI__ = "https://engineering.matterport.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"auroraPage":{"isAuroraPageEnabled":false},"cache":{"experimentGroupSet":true,"reason":"This request is not using the cache middleware worker","group":"disabled","tags":["group-edgeCachePosts","post-7c761e238b46","user-1a69ae209bc4","collection-e3a700e43a0d"],"serverVariantState":"","middlewareEnabled":false,"cacheStatus":"DYNAMIC","shouldUseCache":false,"vary":[],"inDisabledExperiment":false},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":true,"isFirefox":false,"routingEntity":{"type":"COLLECTION","id":"e3a700e43a0d","explicit":true},"viewerIsBot":false},"debug":{"requestId":"0c8a2e37-2b9a-4332-82f6-94c517ec758a","hybridDevServices":[],"originalSpanCarrier":{"ot-tracer-spanid":"43669b766d2eccf8","ot-tracer-traceid":"2c01e720b8ea4826","ot-tracer-sampled":"true"}},"meter":{},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Fengineering.matterport.com\u002Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46","host":"engineering.matterport.com","hostname":"engineering.matterport.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"config":{"nodeEnv":"production","version":"main-20220825-212037-fc552aa4bb","target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20220825-212037-fc552aa4bb","commit":"fc552aa4bb8f1c7cfa1f94dc6c17692a052905e1"}},"datacenter":"us"},"googleAnalyticsCode":"UA-24232453-2","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium","instanceMerchantId":"13685562959212738550"},"applePay":{"version":3},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","braintree":{"enabled":true,"merchantId":"m56f8fqpf7ngnrd4","merchantAccountId":{"usd":"AMediumCorporation_instant","eur":"amediumcorporation_EUR","cad":"amediumcorporation_CAD"},"publicKey":"ds2nn34bg2z7j5gd","braintreeEnvironment":"production","dashboardUrl":"https:\u002F\u002Fwww.braintreegateway.com\u002Fmerchants","gracePeriodDurationInDays":14,"mediumMembershipPlanId":{"monthly":"ce105f8c57a3","monthlyWithTrial":"d5ee3dbe3db8","yearly":"a40ad4a43185","yearlyStaff":"d74fb811198a","yearlyWithTrial":"b3bc7350e5c7","monthlyCad":"p52orjkaceei","yearlyCad":"h4q9g2up9ktt"},"braintreeDiscountId":{"oneMonthFree":"MONTHS_FREE_01","threeMonthsFree":"MONTHS_FREE_03","sixMonthsFree":"MONTHS_FREE_06"},"3DSecureVersion":"2","defaultCurrency":"usd","providerPlanIdCurrency":{"4ycw":"usd","rz3b":"usd","3kqm":"usd","jzw6":"usd","c2q2":"usd","nnsw":"usd","fx7w":"cad","nwf2":"cad"}},"paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","paypal":{"host":"https:\u002F\u002Fapi.paypal.com:443","clientMode":"production","serverMode":"live","webhookId":"4G466076A0294510S","monthlyPlan":{"planId":"P-9WR0658853113943TMU5FDQA","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlan":{"planId":"P-7N8963881P8875835MU5JOPQ","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oneYearGift":{"name":"Medium Membership (1 Year, Digital Gift Code)","description":"Unlimited access to the best and brightest stories on Medium. Gift codes can be redeemed at medium.com\u002Fredeem.","price":"50.00","currency":"USD","sku":"membership-gift-1-yr"},"oldMonthlyPlan":{"planId":"P-96U02458LM656772MJZUVH2Y","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlan":{"planId":"P-59P80963JF186412JJZU3SMI","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"monthlyPlanWithTrial":{"planId":"P-66C21969LR178604GJPVKUKY","name":"Medium Membership (Monthly) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"yearlyPlanWithTrial":{"planId":"P-6XW32684EX226940VKCT2MFA","name":"Medium Membership (Annual) with setup fee","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"oldMonthlyPlanNoSetupFee":{"planId":"P-4N046520HR188054PCJC7LJI","name":"Medium Membership (Monthly)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed monthly."},"oldYearlyPlanNoSetupFee":{"planId":"P-7A4913502Y5181304CJEJMXQ","name":"Medium Membership (Annual)","description":"Unlimited access to the best and brightest stories on Medium. Membership billed annually."},"sdkUrl":"https:\u002F\u002Fwww.paypal.com\u002Fsdk\u002Fjs"},"stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl","log":{"json":true,"level":"info"}},"session":{"xsrf":""}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","viewer":null,"variantFlags":[{"__typename":"VariantFlag","name":"enable_aurora_tag_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_reading_history","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"author_fair_distribution_non_qp3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_price_crossout","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_signup_friction","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_annual_renewal_reminder_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_evhead_com_to_ev_medium_com_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_regwall_email_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefined_top_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_import","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_reply_to_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hightower_user_bonus","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"explicit_signals_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"enable_creator_welcome_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tag_recs","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_homepage_for_selected_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mps_creator_growth_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"pub_sidebar","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_sign_in_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_seamless_social_sharing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_email_sign_in_captcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pill_based_home_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_updated_follower_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_social_share_sheet","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_responses_rewrite","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"reader_fair_distribution_non_qp","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_generation_pipeline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"textshots_userid","valueType":{"__typename":"VariantFlagString","value":""}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_newsletter_lo_flow_custom_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pills_based_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automod","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_speechify_widget","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_twitter_auth_suggestions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_iceland_forced_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_legacy_feed_in_iceland","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"explicit_signals_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_google_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_group_gifting","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_homepage_reading_list","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"enable_new_three_dot_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"can_receive_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_sprig","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"android_two_hour_refresh","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"provider_for_credit_card_form","valueType":{"__typename":"VariantFlagString","value":"BRAINTREE"}},{"__typename":"VariantFlag","name":"enable_updated_new_user_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_domain_v2_settings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"posts_under_quota_fair_distribution","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_dashboard_referred_earnings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_auto_follow_on_subscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_in_app_free_trial","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_response_markup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trumpland_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_creator_about_editor","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipping_v0_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound"}},{"__typename":"VariantFlag","name":"can_send_tips_v0","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_in_context_sharing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_partner_program_enrollment","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"onboarding_tags_from_top_views","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_display_paywall_after_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_fs_cache_user_vals","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"unhide_mobile_ctas","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_triton_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_about_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_highlander_member_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_autorefresh","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"collectionByDomainOrSlug({\"domainOrSlug\":\"engineering.matterport.com\"})":{"__ref":"Collection:e3a700e43a0d"},"postResult({\"id\":\"7c761e238b46\"})":{"__ref":"Post:7c761e238b46"},"recirc({\"paging\":{\"limit\":4},\"postId\":\"7c761e238b46\"})":{"__typename":"RexRecircResult","items":[{"__typename":"RexRecircItem","post":{"__ref":"Post:a00b54245450"},"feedId":"6c28a0c7-8749-4c40-90db-d09ae5242f51"},{"__typename":"RexRecircItem","post":{"__ref":"Post:978026bbcdfb"},"feedId":"6c28a0c7-8749-4c40-90db-d09ae5242f51"},{"__typename":"RexRecircItem","post":{"__ref":"Post:c4c164870f21"},"feedId":"6c28a0c7-8749-4c40-90db-d09ae5242f51"},{"__typename":"RexRecircItem","post":{"__ref":"Post:a6a642271051"},"feedId":"6c28a0c7-8749-4c40-90db-d09ae5242f51"}]}},"ImageMetadata:":{"__typename":"ImageMetadata","id":""},"Collection:e3a700e43a0d":{"__typename":"Collection","id":"e3a700e43a0d","favicon":{"__ref":"ImageMetadata:"},"customStyleSheet":null,"colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FA","point":0},{"__typename":"ColorPoint","color":"#FFE9F3FA","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FA","point":0.2},{"__typename":"ColorPoint","color":"#FFE2F0FA","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEF9","point":0.4},{"__typename":"ColorPoint","color":"#FFDBEDF9","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBF9","point":0.6},{"__typename":"ColorPoint","color":"#FFD3EAF9","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E8F9","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE7F9","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E5F9","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668BA2","point":0},{"__typename":"ColorPoint","color":"#FF608196","point":0.1},{"__typename":"ColorPoint","color":"#FF5A778A","point":0.2},{"__typename":"ColorPoint","color":"#FF536D7D","point":0.3},{"__typename":"ColorPoint","color":"#FF4C6270","point":0.4},{"__typename":"ColorPoint","color":"#FF455864","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4D57","point":0.6},{"__typename":"ColorPoint","color":"#FF344149","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353C","point":0.8},{"__typename":"ColorPoint","color":"#FF21282D","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1E","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF002C40","colorPoints":[{"__typename":"ColorPoint","color":"#FF002C40","point":0},{"__typename":"ColorPoint","color":"#FF27485B","point":0.1},{"__typename":"ColorPoint","color":"#FF446173","point":0.2},{"__typename":"ColorPoint","color":"#FF5E7889","point":0.3},{"__typename":"ColorPoint","color":"#FF778E9D","point":0.4},{"__typename":"ColorPoint","color":"#FF8EA2B0","point":0.5},{"__typename":"ColorPoint","color":"#FFA5B6C2","point":0.6},{"__typename":"ColorPoint","color":"#FFBBC9D4","point":0.7},{"__typename":"ColorPoint","color":"#FFD1DCE5","point":0.8},{"__typename":"ColorPoint","color":"#FFE6EEF5","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"googleAnalyticsId":null,"domain":"engineering.matterport.com","name":"Matterport Engineering Techblog","slug":"matterport-engineering","avatar":{"__ref":"ImageMetadata:1*bSmfQypBkGaCulaTyJk4ew.png"},"isAuroraVisible":false,"legacyHeaderBackgroundImage":{"__ref":"ImageMetadata:1*-GRZXOSwWDTNpaFNHrdsnw.jpeg"},"logo":{"__ref":"ImageMetadata:1*g2BYfefpbBmRV26SMbaUxQ.png"},"subscriberCount":785,"newsletterV3":null,"navItems":[],"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:e3a700e43a0d-viewerId:lo_8e44f571720c"},"creator":{"__ref":"User:d554379f4ffd"},"isAuroraEligible":false,"twitterUsername":"matterport","facebookPageId":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","status":"ACTIVE","isSubdomain":false}},"ptsQualifiedAt":1616092831543,"description":"Thoughts on 3D from Matterport’s web and vision teams."},"UserViewerEdge:userId:1a69ae209bc4-viewerId:lo_8e44f571720c":{"__typename":"UserViewerEdge","id":"userId:1a69ae209bc4-viewerId:lo_8e44f571720c","isFollowing":false,"isUser":false},"NewsletterV3:1f594181b9d7":{"__typename":"NewsletterV3","id":"1f594181b9d7","type":"NEWSLETTER_TYPE_AUTHOR","slug":"1a69ae209bc4","name":"1a69ae209bc4","collection":null,"user":{"__ref":"User:1a69ae209bc4"},"description":"","promoHeadline":"","promoBody":"","showPromo":false,"subscribersCount":2},"User:1a69ae209bc4":{"__typename":"User","id":"1a69ae209bc4","name":"Waleed Abdulla","username":"waleedka","newsletterV3":{"__ref":"NewsletterV3:1f594181b9d7"},"imageId":"0*RDTSqB0ocRKRgYEA.JPG","socialStats":{"__typename":"SocialStats","followerCount":1698,"followingCount":333,"collectionFollowingCount":7},"customStyleSheet":null,"bio":"Startups, deep learning, computer vision.","isPartnerProgramEnrolled":false,"viewerEdge":{"__ref":"UserViewerEdge:userId:1a69ae209bc4-viewerId:lo_8e44f571720c"},"viewerIsUser":false,"postSubscribeMembershipUpsellShownAt":0,"customDomainState":null,"hasSubdomain":false,"mediumMemberAt":0,"about":"","homepagePostsConnection:{\"paging\":{\"limit\":1}}":{"__typename":"PostConnection","posts":[{"__ref":"Post:7c761e238b46"}]},"isSuspended":false,"allowNotes":true,"isAuroraVisible":true,"twitterScreenName":"waleedka","atsQualifiedAt":1612205621523},"Post:7c761e238b46":{"__typename":"Post","id":"7c761e238b46","firstPublishedAt":1521505381092,"visibility":"PUBLIC","creator":{"__ref":"User:1a69ae209bc4"},"canonicalUrl":"","collection":{"__ref":"Collection:e3a700e43a0d"},"content({\"postMeteringOptions\":{\"forceTruncation\":false}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","bodyModel":{"__typename":"RichText","sections":[{"__typename":"Section","name":"4efa","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}],"paragraphs":[{"__ref":"Paragraph:6d767af13c8b_0"},{"__ref":"Paragraph:6d767af13c8b_1"},{"__ref":"Paragraph:6d767af13c8b_2"},{"__ref":"Paragraph:6d767af13c8b_3"},{"__ref":"Paragraph:6d767af13c8b_4"},{"__ref":"Paragraph:6d767af13c8b_5"},{"__ref":"Paragraph:6d767af13c8b_6"},{"__ref":"Paragraph:6d767af13c8b_7"},{"__ref":"Paragraph:6d767af13c8b_8"},{"__ref":"Paragraph:6d767af13c8b_9"},{"__ref":"Paragraph:6d767af13c8b_10"},{"__ref":"Paragraph:6d767af13c8b_11"},{"__ref":"Paragraph:6d767af13c8b_12"},{"__ref":"Paragraph:6d767af13c8b_13"},{"__ref":"Paragraph:6d767af13c8b_14"},{"__ref":"Paragraph:6d767af13c8b_15"},{"__ref":"Paragraph:6d767af13c8b_16"},{"__ref":"Paragraph:6d767af13c8b_17"},{"__ref":"Paragraph:6d767af13c8b_18"},{"__ref":"Paragraph:6d767af13c8b_19"},{"__ref":"Paragraph:6d767af13c8b_20"},{"__ref":"Paragraph:6d767af13c8b_21"},{"__ref":"Paragraph:6d767af13c8b_22"},{"__ref":"Paragraph:6d767af13c8b_23"},{"__ref":"Paragraph:6d767af13c8b_24"},{"__ref":"Paragraph:6d767af13c8b_25"},{"__ref":"Paragraph:6d767af13c8b_26"},{"__ref":"Paragraph:6d767af13c8b_27"},{"__ref":"Paragraph:6d767af13c8b_28"},{"__ref":"Paragraph:6d767af13c8b_29"},{"__ref":"Paragraph:6d767af13c8b_30"},{"__ref":"Paragraph:6d767af13c8b_31"},{"__ref":"Paragraph:6d767af13c8b_32"},{"__ref":"Paragraph:6d767af13c8b_33"},{"__ref":"Paragraph:6d767af13c8b_34"},{"__ref":"Paragraph:6d767af13c8b_35"},{"__ref":"Paragraph:6d767af13c8b_36"},{"__ref":"Paragraph:6d767af13c8b_37"},{"__ref":"Paragraph:6d767af13c8b_38"},{"__ref":"Paragraph:6d767af13c8b_39"},{"__ref":"Paragraph:6d767af13c8b_40"},{"__ref":"Paragraph:6d767af13c8b_41"},{"__ref":"Paragraph:6d767af13c8b_42"},{"__ref":"Paragraph:6d767af13c8b_43"},{"__ref":"Paragraph:6d767af13c8b_44"},{"__ref":"Paragraph:6d767af13c8b_45"},{"__ref":"Paragraph:6d767af13c8b_46"},{"__ref":"Paragraph:6d767af13c8b_47"},{"__ref":"Paragraph:6d767af13c8b_48"},{"__ref":"Paragraph:6d767af13c8b_49"},{"__ref":"Paragraph:6d767af13c8b_50"},{"__ref":"Paragraph:6d767af13c8b_51"},{"__ref":"Paragraph:6d767af13c8b_52"},{"__ref":"Paragraph:6d767af13c8b_53"},{"__ref":"Paragraph:6d767af13c8b_54"},{"__ref":"Paragraph:6d767af13c8b_55"},{"__ref":"Paragraph:6d767af13c8b_56"},{"__ref":"Paragraph:6d767af13c8b_57"},{"__ref":"Paragraph:6d767af13c8b_58"},{"__ref":"Paragraph:6d767af13c8b_59"},{"__ref":"Paragraph:6d767af13c8b_60"},{"__ref":"Paragraph:6d767af13c8b_61"},{"__ref":"Paragraph:6d767af13c8b_62"},{"__ref":"Paragraph:6d767af13c8b_63"},{"__ref":"Paragraph:6d767af13c8b_64"},{"__ref":"Paragraph:6d767af13c8b_65"},{"__ref":"Paragraph:6d767af13c8b_66"},{"__ref":"Paragraph:6d767af13c8b_67"},{"__ref":"Paragraph:6d767af13c8b_68"},{"__ref":"Paragraph:6d767af13c8b_69"},{"__ref":"Paragraph:6d767af13c8b_70"},{"__ref":"Paragraph:6d767af13c8b_71"},{"__ref":"Paragraph:6d767af13c8b_72"},{"__ref":"Paragraph:6d767af13c8b_73"},{"__ref":"Paragraph:6d767af13c8b_74"},{"__ref":"Paragraph:6d767af13c8b_75"},{"__ref":"Paragraph:6d767af13c8b_76"},{"__ref":"Paragraph:6d767af13c8b_77"},{"__ref":"Paragraph:6d767af13c8b_78"},{"__ref":"Paragraph:6d767af13c8b_79"},{"__ref":"Paragraph:6d767af13c8b_80"},{"__ref":"Paragraph:6d767af13c8b_81"},{"__ref":"Paragraph:6d767af13c8b_82"},{"__ref":"Paragraph:6d767af13c8b_83"},{"__ref":"Paragraph:6d767af13c8b_84"},{"__ref":"Paragraph:6d767af13c8b_85"},{"__ref":"Paragraph:6d767af13c8b_86"},{"__ref":"Paragraph:6d767af13c8b_87"},{"__ref":"Paragraph:6d767af13c8b_88"},{"__ref":"Paragraph:6d767af13c8b_89"},{"__ref":"Paragraph:6d767af13c8b_90"},{"__ref":"Paragraph:6d767af13c8b_91"},{"__ref":"Paragraph:6d767af13c8b_92"},{"__ref":"Paragraph:6d767af13c8b_93"},{"__ref":"Paragraph:6d767af13c8b_94"},{"__ref":"Paragraph:6d767af13c8b_95"},{"__ref":"Paragraph:6d767af13c8b_96"},{"__ref":"Paragraph:6d767af13c8b_97"},{"__ref":"Paragraph:6d767af13c8b_98"},{"__ref":"Paragraph:6d767af13c8b_99"},{"__ref":"Paragraph:6d767af13c8b_100"},{"__ref":"Paragraph:6d767af13c8b_101"},{"__ref":"Paragraph:6d767af13c8b_102"},{"__ref":"Paragraph:6d767af13c8b_103"},{"__ref":"Paragraph:6d767af13c8b_104"},{"__ref":"Paragraph:6d767af13c8b_105"},{"__ref":"Paragraph:6d767af13c8b_106"},{"__ref":"Paragraph:6d767af13c8b_107"},{"__ref":"Paragraph:6d767af13c8b_108"},{"__ref":"Paragraph:6d767af13c8b_109"},{"__ref":"Paragraph:6d767af13c8b_110"},{"__ref":"Paragraph:6d767af13c8b_111"},{"__ref":"Paragraph:6d767af13c8b_112"},{"__ref":"Paragraph:6d767af13c8b_113"},{"__ref":"Paragraph:6d767af13c8b_114"},{"__ref":"Paragraph:6d767af13c8b_115"},{"__ref":"Paragraph:6d767af13c8b_116"},{"__ref":"Paragraph:6d767af13c8b_117"},{"__ref":"Paragraph:6d767af13c8b_118"},{"__ref":"Paragraph:6d767af13c8b_119"},{"__ref":"Paragraph:6d767af13c8b_120"}]}},"customStyleSheet":null,"isPublished":true,"isLocked":false,"license":"ALL_RIGHTS_RESERVED","collaborators":[],"isMarkedPaywallOnly":false,"lockedSource":"LOCKED_POST_SOURCE_NONE","mediumUrl":"https:\u002F\u002Fengineering.matterport.com\u002Fsplash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46","latestPublishedVersion":"6d767af13c8b","postResponses":{"__typename":"PostResponses","count":69},"allowResponses":true,"isLimitedState":false,"voterCount":1330,"recommenders":[],"title":"Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow","clapCount":7370,"statusForCollection":"APPROVED","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":0,"responseDistribution":"NOT_DISTRIBUTED","inResponseToPostResult":null,"inResponseToCatalogResult":null,"pendingCollection":null,"isNewsletter":false,"isAuthorNewsletter":false,"layerCake":0,"tags":[{"__ref":"Tag:mask-rcnn"},{"__ref":"Tag:object-detection"},{"__ref":"Tag:instance-segmentation"},{"__ref":"Tag:computer-vision"},{"__ref":"Tag:deep-learning"}],"topics":[{"__typename":"Topic","name":"Machine Learning"}],"sequence":null,"readingTime":11.84811320754717,"inResponseToEntityType":null,"isSeries":false,"uniqueSlug":"splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46","primaryTopic":null,"socialTitle":"","socialDek":"","noIndex":null,"curationStatus":"CURATION_STATUS_DISABLED","metaDescription":"","latestPublishedAt":1544406800068,"previewContent":{"__typename":"PreviewContent","subtitle":"Explained by building a color splash filter"},"previewImage":{"__ref":"ImageMetadata:1*w_ownWZZ38QhiVjVU757DA.png"},"isShortform":false,"seoTitle":"","updatedAt":1544406800068,"shortformType":"SHORTFORM_TYPE_LINK","seoDescription":"","isIndexable":true,"isSuspended":false,"responseRootPost":{"__typename":"ResponseRootPost","post":{"__ref":"Post:7c761e238b46"}},"internalLinks({\"paging\":{\"limit\":8}})":{"__typename":"InternalLinksConnection","items":[{"__ref":"Post:60ae243e040a"},{"__ref":"Post:d9819ca49d1c"},{"__ref":"Post:6b043212274d"},{"__ref":"Post:c0645f794352"},{"__ref":"Post:460436ee2393"},{"__ref":"Post:450160f3b43b"},{"__ref":"Post:4bb4f297c672"},{"__ref":"Post:5f2e173e967e"}]},"awards:countToShowAwardBadge(type:STAFF_PICK,limit:1)":{"__typename":"AwardConnection","totalCount":0,"awards":[]}},"User:9c2b31be19c2":{"__typename":"User","id":"9c2b31be19c2","imageId":"1*i4XQzztpUqzvEoEtVvkQOA.jpeg","mediumMemberAt":0,"name":"Sneha Nanavati","username":"bubblegum.frivolous","customDomainState":null,"hasSubdomain":false,"bio":"📚 books 🎥 movies ⚽️ football 📻 21st-century internet hedonist ✏️ garden variety Thoreau making this my Walden 👇https:\u002F\u002Fwww.instagram.com\u002F_bubblegumfactory_\u002F"},"Collection:86b2eab43b1b":{"__typename":"Collection","id":"86b2eab43b1b","name":"AIcrowd","domain":"blog.aicrowd.com","slug":"aicrowd"},"ImageMetadata:1*kwuOdYjO5Xmyi9dm5JUhmw.png":{"__typename":"ImageMetadata","id":"1*kwuOdYjO5Xmyi9dm5JUhmw.png","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:a00b54245450":{"__typename":"Post","id":"a00b54245450","title":"Faces: AI Blitz XIII with Team GLaDOS","mediumUrl":"https:\u002F\u002Fblog.aicrowd.com\u002Ffaces-ai-blitz-xiii-with-team-glados-a00b54245450","creator":{"__ref":"User:9c2b31be19c2"},"previewContent":{"__typename":"PreviewContent","subtitle":"This blog covers the top winning solutions for all the puzzles in Blitz XIII: Faces. It covers the leaderboard winning team GLaDOS and…","isFullContent":false},"collection":{"__ref":"Collection:86b2eab43b1b"},"previewImage":{"__ref":"ImageMetadata:1*kwuOdYjO5Xmyi9dm5JUhmw.png"},"clapCount":15,"isSeries":false,"sequence":null,"uniqueSlug":"faces-ai-blitz-xiii-with-team-glados-a00b54245450","visibility":"PUBLIC"},"User:94a15e80ac0e":{"__typename":"User","id":"94a15e80ac0e","imageId":"1*NCewNVNQJ2XPEWaS3LcO8g.jpeg","mediumMemberAt":0,"name":"Ritesh Kanjee","username":"augmentedstartups","customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"augmentedstartups.medium.com"}},"hasSubdomain":true,"bio":"CEO Augmented Startups — M(Eng) Electronic Engineer, YouTuber 100'000+ Subscribers."},"Collection:e96e88f703e2":{"__typename":"Collection","id":"e96e88f703e2","name":"Augmented Startups","domain":null,"slug":"augmented-startups"},"ImageMetadata:1*HgAslEcX1QP1Nx4wZe1lkA.png":{"__typename":"ImageMetadata","id":"1*HgAslEcX1QP1Nx4wZe1lkA.png","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:978026bbcdfb":{"__typename":"Post","id":"978026bbcdfb","title":"Mean Average Precision (mAP) — Fun & Easy Explanation","mediumUrl":"https:\u002F\u002Fmedium.com\u002Faugmented-startups\u002Fmean-average-precision-map-fun-easy-explanation-978026bbcdfb","creator":{"__ref":"User:94a15e80ac0e"},"previewContent":{"__typename":"PreviewContent","subtitle":"How to Measure Accuracy in Computer Vision?","isFullContent":false},"collection":{"__ref":"Collection:e96e88f703e2"},"previewImage":{"__ref":"ImageMetadata:1*HgAslEcX1QP1Nx4wZe1lkA.png"},"clapCount":15,"isSeries":false,"sequence":null,"uniqueSlug":"mean-average-precision-map-fun-easy-explanation-978026bbcdfb","visibility":"PUBLIC"},"User:24c39747617":{"__typename":"User","id":"24c39747617","imageId":"1*WuMKKoy4qWUenOofNA-Q3g.jpeg","mediumMemberAt":0,"name":"Chinmay Bhalerao","username":"BH_Chinmay","customDomainState":null,"hasSubdomain":false,"bio":"Storyteller | AI-ML Developer | Data Analyst | Computer Vision| Masters in Mathematical Modelling and Simulation"},"ImageMetadata:0*zf-wVE5ZdH3v4IHB":{"__typename":"ImageMetadata","id":"0*zf-wVE5ZdH3v4IHB","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:c4c164870f21":{"__typename":"Post","id":"c4c164870f21","title":"Calibration in Image Processing","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@BH_Chinmay\u002Fcalibration-in-image-processing-c4c164870f21","creator":{"__ref":"User:24c39747617"},"previewContent":{"__typename":"PreviewContent","subtitle":"Many times in image processing and object detection problems, we have to measure sizes of objects from images. In Quality control…","isFullContent":false},"collection":null,"previewImage":{"__ref":"ImageMetadata:0*zf-wVE5ZdH3v4IHB"},"clapCount":67,"isSeries":false,"sequence":null,"uniqueSlug":"calibration-in-image-processing-c4c164870f21","visibility":"PUBLIC"},"User:9fe3c37c813e":{"__typename":"User","id":"9fe3c37c813e","imageId":"0*aj14zg1HdhVibwC_","mediumMemberAt":0,"name":"Vaibhav Bagri","username":"vaibhavbagri07","customDomainState":null,"hasSubdomain":false,"bio":"M.S. in Computer Science @ Columbia University"},"ImageMetadata:0*TpwrVjUKUWL9D36r.jpg":{"__typename":"ImageMetadata","id":"0*TpwrVjUKUWL9D36r.jpg","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:a6a642271051":{"__typename":"Post","id":"a6a642271051","title":"Looking at Research Work in Real Time Object Detection","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@vaibhavbagri07\u002Flooking-at-research-work-in-real-time-object-detection-a6a642271051","creator":{"__ref":"User:9fe3c37c813e"},"previewContent":{"__typename":"PreviewContent","subtitle":"Object detection refers to the ability of computers to accurately detect the presence of particular objects in an image and accurately draw…","isFullContent":false},"collection":null,"previewImage":{"__ref":"ImageMetadata:0*TpwrVjUKUWL9D36r.jpg"},"clapCount":1,"isSeries":false,"sequence":null,"uniqueSlug":"looking-at-research-work-in-real-time-object-detection-a6a642271051","visibility":"PUBLIC"},"ImageMetadata:1*bSmfQypBkGaCulaTyJk4ew.png":{"__typename":"ImageMetadata","id":"1*bSmfQypBkGaCulaTyJk4ew.png"},"ImageMetadata:1*-GRZXOSwWDTNpaFNHrdsnw.jpeg":{"__typename":"ImageMetadata","id":"1*-GRZXOSwWDTNpaFNHrdsnw.jpeg","originalWidth":3394,"focusPercentX":51.42857142857143,"focusPercentY":21.05263157894737},"ImageMetadata:1*g2BYfefpbBmRV26SMbaUxQ.png":{"__typename":"ImageMetadata","id":"1*g2BYfefpbBmRV26SMbaUxQ.png","originalHeight":1376,"originalWidth":3394},"Paragraph:6d767af13c8b_0":{"__typename":"Paragraph","id":"6d767af13c8b_0","name":"f392","type":"H3","href":null,"layout":null,"metadata":null,"text":"Splash of Color: Instance Segmentation with Mask R-CNN and TensorFlow","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_1":{"__typename":"Paragraph","id":"6d767af13c8b_1","name":"fcd0","type":"H4","href":null,"layout":null,"metadata":null,"text":"Explained by building a color splash filter","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_2":{"__typename":"Paragraph","id":"6d767af13c8b_2","name":"04b2","type":"P","href":null,"layout":null,"metadata":null,"text":"Back in November, we open-sourced our implementation of Mask R-CNN, and since then it’s been forked 1400 times, used in a lot of projects, and improved upon by many generous contributors. We received a lot of questions as well, so in this post I’ll explain how the model works and show how to use it in a real application.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":38,"end":66,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_3":{"__typename":"Paragraph","id":"6d767af13c8b_3","name":"64b6","type":"P","href":null,"layout":null,"metadata":null,"text":"I’ll cover two things: First, an overview of Mask RCNN. And, second, how to train a model from scratch and use it to build a smart color splash filter.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_4":{"__typename":"Paragraph","id":"6d767af13c8b_4","name":"3f65","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nWe’re sharing the code here. Including the dataset I built and the trained model. Follow along!","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":33,"end":37,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Ftree\u002Fmaster\u002Fsamples\u002Fballoon","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_5":{"__typename":"Paragraph","id":"6d767af13c8b_5","name":"1d4c","type":"H3","href":null,"layout":null,"metadata":null,"text":"What is Instance Segmentation?","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_6":{"__typename":"Paragraph","id":"6d767af13c8b_6","name":"4e7c","type":"P","href":null,"layout":null,"metadata":null,"text":"Instance segmentation is the task of identifying object outlines at the pixel level. Compared to similar computer vision tasks, it’s one of the hardest possible vision tasks. Consider the following asks:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-zw_Mh1e-8YncnokbAFWxg.png":{"__typename":"ImageMetadata","id":"1*-zw_Mh1e-8YncnokbAFWxg.png","originalHeight":748,"originalWidth":996,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_7":{"__typename":"Paragraph","id":"6d767af13c8b_7","name":"9486","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*-zw_Mh1e-8YncnokbAFWxg.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_8":{"__typename":"Paragraph","id":"6d767af13c8b_8","name":"beac","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Classification: There is a balloon in this image.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":16,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_9":{"__typename":"Paragraph","id":"6d767af13c8b_9","name":"cec8","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Semantic Segmentation: These are all the balloon pixels.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":22,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_10":{"__typename":"Paragraph","id":"6d767af13c8b_10","name":"db7f","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Object Detection: There are 7 balloons in this image at these locations. We’re starting to account for objects that overlap.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":18,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_11":{"__typename":"Paragraph","id":"6d767af13c8b_11","name":"a421","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Instance Segmentation: There are 7 balloons at these locations, and these are the pixels that belong to each one.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_12":{"__typename":"Paragraph","id":"6d767af13c8b_12","name":"fb1b","type":"H3","href":null,"layout":null,"metadata":null,"text":"Mask R-CNN","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_13":{"__typename":"Paragraph","id":"6d767af13c8b_13","name":"4745","type":"P","href":null,"layout":null,"metadata":null,"text":"Mask R-CNN (regional convolutional neural network) is a two stage framework: the first stage scans the image and generates proposals(areas likely to contain an object). And the second stage classifies the proposals and generates bounding boxes and masks.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":123,"end":132,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_14":{"__typename":"Paragraph","id":"6d767af13c8b_14","name":"7305","type":"P","href":null,"layout":null,"metadata":null,"text":"It was introduced last year via the Mask R-CNN paper to extend its predecessor, Faster R-CNN, by the same authors. Faster R-CNN is a popular framework for object detection, and Mask R-CNN extends it with instance segmentation, among other things.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":36,"end":52,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1703.06870","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":80,"end":92,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1506.01497","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*IWWOPIYLqqF9i_gXPmBk3g.png":{"__typename":"ImageMetadata","id":"1*IWWOPIYLqqF9i_gXPmBk3g.png","originalHeight":579,"originalWidth":1285,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_15":{"__typename":"Paragraph","id":"6d767af13c8b_15","name":"2b38","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*IWWOPIYLqqF9i_gXPmBk3g.png"},"text":"Mask R-CNN framework. Source: https:\u002F\u002Farxiv.org\u002Fabs\u002F1703.06870","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":30,"end":62,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1703.06870","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_16":{"__typename":"Paragraph","id":"6d767af13c8b_16","name":"9820","type":"P","href":null,"layout":null,"metadata":null,"text":"At a high level, Mask R-CNN consists of these modules:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_17":{"__typename":"Paragraph","id":"6d767af13c8b_17","name":"5d32","type":"H3","href":null,"layout":null,"metadata":null,"text":"1. Backbone","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*IDjLXsSw5QMFWDudayIBfw.png":{"__typename":"ImageMetadata","id":"1*IDjLXsSw5QMFWDudayIBfw.png","originalHeight":270,"originalWidth":309,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_18":{"__typename":"Paragraph","id":"6d767af13c8b_18","name":"3311","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*IDjLXsSw5QMFWDudayIBfw.png"},"text":"Simplified illustration of the backbone nework","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_19":{"__typename":"Paragraph","id":"6d767af13c8b_19","name":"b9da","type":"P","href":null,"layout":null,"metadata":null,"text":"This is a standard convolutional neural network (typically, ResNet50 or ResNet101) that serves as a feature extractor. The early layers detect low level features (edges and corners), and later layers successively detect higher level features (car, person, sky).","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_20":{"__typename":"Paragraph","id":"6d767af13c8b_20","name":"5ab7","type":"P","href":null,"layout":null,"metadata":null,"text":"Passing through the backbone network, the image is converted from 1024x1024px x 3 (RGB) to a feature map of shape 32x32x2048. This feature map becomes the input for the following stages.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_21":{"__typename":"Paragraph","id":"6d767af13c8b_21","name":"b6fc","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nThe backbone is built in the function resnet_graph(). The code supports ResNet50 and ResNet101.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":48,"end":62,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fmodel.py#L171","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_22":{"__typename":"Paragraph","id":"6d767af13c8b_22","name":"62a8","type":"H4","href":null,"layout":null,"metadata":null,"text":"Feature Pyramid Network","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*1sCveJrqfthOQsGGZRs2tQ.png":{"__typename":"ImageMetadata","id":"1*1sCveJrqfthOQsGGZRs2tQ.png","originalHeight":177,"originalWidth":452,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_23":{"__typename":"Paragraph","id":"6d767af13c8b_23","name":"78cb","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*1sCveJrqfthOQsGGZRs2tQ.png"},"text":"Source: Feature Pyramid Networks paper","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_24":{"__typename":"Paragraph","id":"6d767af13c8b_24","name":"3947","type":"P","href":null,"layout":null,"metadata":null,"text":"While the backbone described above works great, it can be improved upon. The Feature Pyramid Network (FPN) was introduced by the same authors of Mask R-CNN as an extension that can better represent objects at multiple scales.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":77,"end":106,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1612.03144","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_25":{"__typename":"Paragraph","id":"6d767af13c8b_25","name":"4b58","type":"P","href":null,"layout":null,"metadata":null,"text":"FPN improves the standard feature extraction pyramid by adding a second pyramid that takes the high level features from the first pyramid and passes them down to lower layers. By doing so, it allows features at every level to have access to both, lower and higher level features.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_26":{"__typename":"Paragraph","id":"6d767af13c8b_26","name":"2946","type":"P","href":null,"layout":null,"metadata":null,"text":"Our implementation of Mask RCNN uses a ResNet101 + FPN backbone.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_27":{"__typename":"Paragraph","id":"6d767af13c8b_27","name":"2994","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\n The FPN is created in MaskRCNN.build(). The section after building the ResNet. \nRPN introduces additional complexity: rather than a single backbone feature map in the standard backbone (i.e. the top layer of the first pyramid), in FPN there is a feature map at each level of the second pyramid. We pick which to use dynamically depending on the size of the object. I’ll continue to refer to the backbone feature map as if it’s one feature map, but keep in mind that when using FPN, we’re actually picking one out of several at runtime.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":33,"end":49,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fmodel.py#L1840","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":406,"end":426,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_28":{"__typename":"Paragraph","id":"6d767af13c8b_28","name":"1cdb","type":"H3","href":null,"layout":null,"metadata":null,"text":"2. Region Proposal Network (RPN)","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*ESpJx0XLvyBa86TNo2BfLQ.png":{"__typename":"ImageMetadata","id":"1*ESpJx0XLvyBa86TNo2BfLQ.png","originalHeight":578,"originalWidth":593,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_29":{"__typename":"Paragraph","id":"6d767af13c8b_29","name":"c7e9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*ESpJx0XLvyBa86TNo2BfLQ.png"},"text":"Simplified illustration showing 49 anchor boxes","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_30":{"__typename":"Paragraph","id":"6d767af13c8b_30","name":"465d","type":"P","href":null,"layout":null,"metadata":null,"text":"The RPN is a lightweight neural network that scans the image in a sliding-window fashion and finds areas that contain objects.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_31":{"__typename":"Paragraph","id":"6d767af13c8b_31","name":"ab7f","type":"P","href":null,"layout":null,"metadata":null,"text":"The regions that the RPN scans over are called anchors. Which are boxes distributed over the image area, as show on the left. This is a simplified view, though. In practice, there are about 200K anchors of different sizes and aspect ratios, and they overlap to cover as much of the image as possible.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":47,"end":54,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_32":{"__typename":"Paragraph","id":"6d767af13c8b_32","name":"8557","type":"P","href":null,"layout":null,"metadata":null,"text":"How fast can the RPN scan that many anchors? Pretty fast, actually. The sliding window is handled by the convolutional nature of the RPN, which allows it to scan all regions in parallel (on a GPU). Further, the RPN doesn’t scan over the image directly (even though we draw the anchors on the image for illustration). Instead, the RPN scans over the backbone feature map. This allows the RPN to reuse the extracted features efficiently and avoid duplicate calculations. With these optimizations, the RPN runs in about 10 ms according to the Faster RCNN paper that introduced it. In Mask RCNN we typically use larger images and more anchors, so it might take a bit longer.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":540,"end":557,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1506.01497","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_33":{"__typename":"Paragraph","id":"6d767af13c8b_33","name":"dbe2","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nThe RPN is created in rpn_graph(). Anchor scales and aspect ratios are controlled by RPN_ANCHOR_SCALES and RPN_ANCHOR_RATIOS in config.py.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":32,"end":43,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fmodel.py#L831","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":138,"end":147,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fconfig.py","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_34":{"__typename":"Paragraph","id":"6d767af13c8b_34","name":"384b","type":"P","href":null,"layout":null,"metadata":null,"text":"The RPN generates two outputs for each anchor:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*EMNE8bxOT4RI3HMjIqjCwQ.png":{"__typename":"ImageMetadata","id":"1*EMNE8bxOT4RI3HMjIqjCwQ.png","originalHeight":333,"originalWidth":407,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_35":{"__typename":"Paragraph","id":"6d767af13c8b_35","name":"94b9","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*EMNE8bxOT4RI3HMjIqjCwQ.png"},"text":"3 anchor boxes (dotted) and the shift\u002Fscale applied to them to fit the object precisely (solid). Several anchors can map to the same object.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_36":{"__typename":"Paragraph","id":"6d767af13c8b_36","name":"e802","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Anchor Class: One of two classes: foreground or background. The FG class implies that there is likely an object in that box.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_37":{"__typename":"Paragraph","id":"6d767af13c8b_37","name":"6e7e","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Bounding Box Refinement: A foreground anchor (also called positive anchor) might not be centered perfectly over the object. So the RPN estimates a delta (% change in x, y, width, height) to refine the anchor box to fit the object better.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_38":{"__typename":"Paragraph","id":"6d767af13c8b_38","name":"bb7a","type":"P","href":null,"layout":null,"metadata":null,"text":"Using the RPN predictions, we pick the top anchors that are likely to contain objects and refine their location and size. If several anchors overlap too much, we keep the one with the highest foreground score and discard the rest (referred to as Non-max Suppression). After that we have the final proposals (regions of interest) that we pass to the next stage.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":297,"end":307,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":328,"end":329,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_39":{"__typename":"Paragraph","id":"6d767af13c8b_39","name":"4b8c","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nThe ProposalLayer is a custom Keras layer that reads the output of the RPN, picks top anchors, and applies bounding box refinement.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":14,"end":27,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fmodel.py#L255","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_40":{"__typename":"Paragraph","id":"6d767af13c8b_40","name":"1c01","type":"H3","href":null,"layout":null,"metadata":null,"text":"3. ROI Classifier & Bounding Box Regressor","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_41":{"__typename":"Paragraph","id":"6d767af13c8b_41","name":"cdfb","type":"P","href":null,"layout":null,"metadata":null,"text":"This stage runs on the regions of interest (ROIs) proposed by the RPN. And just like the RPN, it generates two outputs for each ROI:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*xQYuM_9mu5kt8nNN8Ms2TQ.png":{"__typename":"ImageMetadata","id":"1*xQYuM_9mu5kt8nNN8Ms2TQ.png","originalHeight":357,"originalWidth":933,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_42":{"__typename":"Paragraph","id":"6d767af13c8b_42","name":"7aee","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*xQYuM_9mu5kt8nNN8Ms2TQ.png"},"text":"Illustration of stage 2. Source: Fast R-CNN (https:\u002F\u002Farxiv.org\u002Fabs\u002F1504.08083)","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":45,"end":77,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1504.08083","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_43":{"__typename":"Paragraph","id":"6d767af13c8b_43","name":"64f4","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Class: The class of the object in the ROI. Unlike the RPN, which has two classes (FG\u002FBG), this network is deeper and has the capacity to classify regions to specific classes (person, car, chair, …etc.). It can also generate a background class, which causes the ROI to be discarded.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":226,"end":236,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_44":{"__typename":"Paragraph","id":"6d767af13c8b_44","name":"77b5","type":"OLI","href":null,"layout":null,"metadata":null,"text":"Bounding Box Refinement: Very similar to how it’s done in the RPN, and its purpose is to further refine the location and size of the bounding box to encapsulate the object.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":24,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_45":{"__typename":"Paragraph","id":"6d767af13c8b_45","name":"a79e","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nThe classifier and bounding box regressor are created in fpn_classifier_graph().","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":67,"end":89,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fmodel.py#L901","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_46":{"__typename":"Paragraph","id":"6d767af13c8b_46","name":"6611","type":"H4","href":null,"layout":null,"metadata":null,"text":"ROI Pooling","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_47":{"__typename":"Paragraph","id":"6d767af13c8b_47","name":"01b2","type":"P","href":null,"layout":null,"metadata":null,"text":"There is a bit of a problem to solve before we continue. Classifiers don’t handle variable input size very well. They typically require a fixed input size. But, due to the bounding box refinement step in the RPN, the ROI boxes can have different sizes. That’s where ROI Pooling comes into play.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*bsT00ickNk7vaRJNrTvKPQ.png":{"__typename":"ImageMetadata","id":"1*bsT00ickNk7vaRJNrTvKPQ.png","originalHeight":217,"originalWidth":645,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_48":{"__typename":"Paragraph","id":"6d767af13c8b_48","name":"2454","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*bsT00ickNk7vaRJNrTvKPQ.png"},"text":"The feature map here is from a low-level layer, for illustration, to make it easier to understand.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_49":{"__typename":"Paragraph","id":"6d767af13c8b_49","name":"cb1d","type":"P","href":null,"layout":null,"metadata":null,"text":"ROI pooling refers to cropping a part of a feature map and resizing it to a fixed size. It’s similar in principle to cropping part of an image and then resizing it (but there are differences in implementation details).","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_50":{"__typename":"Paragraph","id":"6d767af13c8b_50","name":"3ba3","type":"P","href":null,"layout":null,"metadata":null,"text":"The authors of Mask R-CNN suggest a method they named ROIAlign, in which they sample the feature map at different points and apply a bilinear interpolation. In our implementation, we used TensorFlow’s crop_and_resize function for simplicity and because it’s close enough for most purposes.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":201,"end":216,"href":"https:\u002F\u002Fwww.tensorflow.org\u002Fapi_docs\u002Fpython\u002Ftf\u002Fimage\u002Fcrop_and_resize","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_51":{"__typename":"Paragraph","id":"6d767af13c8b_51","name":"5e88","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nROI pooling is implemented in the class PyramidROIAlign.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":50,"end":65,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fmodel.py#L344","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_52":{"__typename":"Paragraph","id":"6d767af13c8b_52","name":"814c","type":"H3","href":null,"layout":null,"metadata":null,"text":"4. Segmentation Masks","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_53":{"__typename":"Paragraph","id":"6d767af13c8b_53","name":"0414","type":"P","href":null,"layout":null,"metadata":null,"text":"If you stop at the end of the last section then you have a Faster R-CNN framework for object detection. The mask network is the addition that the Mask R-CNN paper introduced.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":59,"end":71,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1506.01497","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*l55WzUq1ZD2b5EGwW05LDA.png":{"__typename":"ImageMetadata","id":"1*l55WzUq1ZD2b5EGwW05LDA.png","originalHeight":201,"originalWidth":455,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_54":{"__typename":"Paragraph","id":"6d767af13c8b_54","name":"7167","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*l55WzUq1ZD2b5EGwW05LDA.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_55":{"__typename":"Paragraph","id":"6d767af13c8b_55","name":"46db","type":"P","href":null,"layout":null,"metadata":null,"text":"The mask branch is a convolutional network that takes the positive regions selected by the ROI classifier and generates masks for them. The generated masks are low resolution: 28x28 pixels. But they are soft masks, represented by float numbers, so they hold more details than binary masks. The small mask size helps keep the mask branch light. During training, we scale down the ground-truth masks to 28x28 to compute the loss, and during inferencing we scale up the predicted masks to the size of the ROI bounding box and that gives us the final masks, one per object.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"EM","start":203,"end":207,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_56":{"__typename":"Paragraph","id":"6d767af13c8b_56","name":"63ed","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nThe mask branch is in build_fpn_mask_graph().","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":32,"end":54,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fmodel.py#L957","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_57":{"__typename":"Paragraph","id":"6d767af13c8b_57","name":"4f9e","type":"H3","href":null,"layout":null,"metadata":null,"text":"Let’s Build a Color Splash Filter","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*lAP6vX1tLQaxFn6XGEQ32g.gif":{"__typename":"ImageMetadata","id":"1*lAP6vX1tLQaxFn6XGEQ32g.gif","originalHeight":444,"originalWidth":460,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_58":{"__typename":"Paragraph","id":"6d767af13c8b_58","name":"4bc3","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*lAP6vX1tLQaxFn6XGEQ32g.gif"},"text":"Sample generated by this project","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_59":{"__typename":"Paragraph","id":"6d767af13c8b_59","name":"e4db","type":"P","href":null,"layout":null,"metadata":null,"text":"Unlike most image editing apps that include this filter, our filter will be a bit smarter: It finds the objects automatically. Which becomes even more useful if you want to apply it to videos rather than a single image.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_60":{"__typename":"Paragraph","id":"6d767af13c8b_60","name":"89d9","type":"H3","href":null,"layout":null,"metadata":null,"text":"Training Dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_61":{"__typename":"Paragraph","id":"6d767af13c8b_61","name":"7371","type":"P","href":null,"layout":null,"metadata":null,"text":"Typically, I’d start by searching for public datasets that contain the objects I need. But in this case, I wanted to document the full cycle and show how to build a dataset from scratch.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_62":{"__typename":"Paragraph","id":"6d767af13c8b_62","name":"dfe0","type":"P","href":null,"layout":null,"metadata":null,"text":"I searched for balloon images on flickr, limiting the license type to “Commercial use & mods allowed”. This returned more than enough images for my needs. I picked a total of 75 images and divided them into a training set and a validation set. Finding images is easy. Annotating them is the hard part.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Q4tCdhwrklvJLM9zn5aDhg.png":{"__typename":"ImageMetadata","id":"1*Q4tCdhwrklvJLM9zn5aDhg.png","originalHeight":673,"originalWidth":871,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_63":{"__typename":"Paragraph","id":"6d767af13c8b_63","name":"66df","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Q4tCdhwrklvJLM9zn5aDhg.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_64":{"__typename":"Paragraph","id":"6d767af13c8b_64","name":"2c9d","type":"P","href":null,"layout":null,"metadata":null,"text":"Wait! Don’t we need, like, a million images to train a deep learning model? Sometimes you do, but often you don’t. I’m relying on two main points to reduce my training requirements significantly:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_65":{"__typename":"Paragraph","id":"6d767af13c8b_65","name":"879c","type":"P","href":null,"layout":null,"metadata":null,"text":"First, transfer learning. Which simply means that, instead of training a model from scratch, I start with a weights file that’s been trained on the COCO dataset (we provide that in the github repo). Although the COCO dataset does not contain a balloon class, it contains a lot of other images (~120K), so the trained weights have already learned a lot of the features common in natural images, which really helps. And, second, given the simple use case here, I’m not demanding high accuracy from this model, so the tiny dataset should suffice.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":230,"end":233,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"EM","start":7,"end":26,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_66":{"__typename":"Paragraph","id":"6d767af13c8b_66","name":"416f","type":"P","href":null,"layout":null,"metadata":null,"text":"There are a lot of tools to annotate images. I ended up using VIA (VGG Image Annotator) because of its simplicity. It’s a single HTML file that you download and open in a browser. Annotating the first few images was very slow, but once I got used to the user interface, I was annotating at around an object a minute.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":62,"end":87,"href":"http:\u002F\u002Fwww.robots.ox.ac.uk\u002F~vgg\u002Fsoftware\u002Fvia\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*6SICkQA-YCLp88A7GFM4Ag.png":{"__typename":"ImageMetadata","id":"1*6SICkQA-YCLp88A7GFM4Ag.png","originalHeight":661,"originalWidth":948,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_67":{"__typename":"Paragraph","id":"6d767af13c8b_67","name":"42e5","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*6SICkQA-YCLp88A7GFM4Ag.png"},"text":"UI of the VGG Image Annotator tool","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_68":{"__typename":"Paragraph","id":"6d767af13c8b_68","name":"a07c","type":"P","href":null,"layout":null,"metadata":null,"text":"If you don’t like the VIA tool, here is a list of the other tools I tested:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_69":{"__typename":"Paragraph","id":"6d767af13c8b_69","name":"4e5e","type":"ULI","href":null,"layout":null,"metadata":null,"text":"LabelMe: One of the most known tools. The UI was a bit too slow, though, especially when zooming in on large images.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":7,"href":"http:\u002F\u002Flabelme2.csail.mit.edu\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_70":{"__typename":"Paragraph","id":"6d767af13c8b_70","name":"f464","type":"ULI","href":null,"layout":null,"metadata":null,"text":"RectLabel: Simple and easy to work with. Mac only.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":9,"href":"https:\u002F\u002Frectlabel.com\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_71":{"__typename":"Paragraph","id":"6d767af13c8b_71","name":"7ae8","type":"ULI","href":null,"layout":null,"metadata":null,"text":"LabelBox: Pretty good for larger labeling projects and has options for different types of labeling tasks.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":8,"href":"https:\u002F\u002Fwww.labelbox.io\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_72":{"__typename":"Paragraph","id":"6d767af13c8b_72","name":"2e31","type":"ULI","href":null,"layout":null,"metadata":null,"text":"VGG Image Annotator (VIA): Fast, light, and really well designed. This is the one I ended up using.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":25,"href":"http:\u002F\u002Fwww.robots.ox.ac.uk\u002F~vgg\u002Fsoftware\u002Fvia\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_73":{"__typename":"Paragraph","id":"6d767af13c8b_73","name":"8ca5","type":"ULI","href":null,"layout":null,"metadata":null,"text":"COCO UI: The tool used to annotate the COCO dataset.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":0,"end":7,"href":"https:\u002F\u002Fgithub.com\u002Ftylin\u002Fcoco-ui","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_74":{"__typename":"Paragraph","id":"6d767af13c8b_74","name":"d280","type":"H3","href":null,"layout":null,"metadata":null,"text":"Loading the Dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_75":{"__typename":"Paragraph","id":"6d767af13c8b_75","name":"a153","type":"P","href":null,"layout":null,"metadata":null,"text":"There isn’t a universally accepted format to store segmentation masks. Some datasets save them as PNG images, others store them as polygon points, and so on. To handle all these cases, our implementation provides a Dataset class that you inherit from and then override a few functions to read your data in whichever format it happens to be.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_76":{"__typename":"Paragraph","id":"6d767af13c8b_76","name":"b712","type":"P","href":null,"layout":null,"metadata":null,"text":"The VIA tool saves the annotations in a JSON file, and each mask is a set of polygon points. I didn’t find documentation for the format, but it’s pretty easy to figure out by looking at the generated JSON. I included comments in the code to explain how the parsing is done.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_77":{"__typename":"Paragraph","id":"6d767af13c8b_77","name":"e128","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nAn easy way to write code for a new dataset is to copy coco.py and modify it to your needs. Which is what I did. I saved the new file as balloons.py","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":65,"end":72,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fsamples\u002Fcoco\u002Fcoco.py","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":147,"end":158,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fv2.1\u002Fsamples\u002Fballoon\u002Fballoon.py","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_78":{"__typename":"Paragraph","id":"6d767af13c8b_78","name":"2869","type":"P","href":null,"layout":null,"metadata":null,"text":"My BalloonDataset class looks like this:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":3,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_79":{"__typename":"Paragraph","id":"6d767af13c8b_79","name":"71f6","type":"PRE","href":null,"layout":null,"metadata":null,"text":"class BalloonDataset(utils.Dataset):","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":6,"end":20,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_80":{"__typename":"Paragraph","id":"6d767af13c8b_80","name":"1d9c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    def load_balloons(self, dataset_dir, subset):\n        ...","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":8,"end":21,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_81":{"__typename":"Paragraph","id":"6d767af13c8b_81","name":"5c60","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    def load_mask(self, image_id):\n        ...","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":8,"end":17,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_82":{"__typename":"Paragraph","id":"6d767af13c8b_82","name":"a411","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    def image_reference(self, image_id):\n        ...","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":8,"end":23,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_83":{"__typename":"Paragraph","id":"6d767af13c8b_83","name":"2f1b","type":"P","href":null,"layout":null,"metadata":null,"text":"load_balloons reads the JSON file, extracts the annotations, and iteratively calls the internal add_class and add_image functions to build the dataset.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":13,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":96,"end":105,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":110,"end":119,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_84":{"__typename":"Paragraph","id":"6d767af13c8b_84","name":"fd34","type":"BQ","href":null,"layout":null,"metadata":null,"text":"load_mask generates bitmap masks for every object in the image by drawing the polygons.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_85":{"__typename":"Paragraph","id":"6d767af13c8b_85","name":"7af2","type":"P","href":null,"layout":null,"metadata":null,"text":"image_reference simply returns a string that identifies the image for debugging purposes. Here it simply returns the path of the image file.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":0,"end":15,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_86":{"__typename":"Paragraph","id":"6d767af13c8b_86","name":"0866","type":"P","href":null,"layout":null,"metadata":null,"text":"You might have noticed that my class doesn’t contain functions to load images or return bounding boxes. The default load_image function in the base Dataset class handles loading images. And, bounding boxes are generated dynamically from the masks.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":116,"end":126,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":148,"end":155,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_87":{"__typename":"Paragraph","id":"6d767af13c8b_87","name":"dfeb","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nYour dataset might not be in JSON. My BalloonDataset class reads JSON because that’s what the VIA tool generates. Don’t convert your dataset to a format similar to COCO or the VIA format. Insetad, write your own Dataset class to load whichever format your dataset comes in. See the samples and notice how each uses its own Dataset class.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":292,"end":299,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Ftree\u002Fmaster\u002Fsamples","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_88":{"__typename":"Paragraph","id":"6d767af13c8b_88","name":"58a4","type":"H4","href":null,"layout":null,"metadata":null,"text":"Verify the Dataset","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_89":{"__typename":"Paragraph","id":"6d767af13c8b_89","name":"05e7","type":"P","href":null,"layout":null,"metadata":null,"text":"To verify that my new code is implemented correctly I added this Jupyter notebook. It loads the dataset, visualizes masks and bounding boxes, and visualizes the anchors to verify that my anchor sizes are a good fit for my object sizes. Here is an example of what you should expect to see:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":65,"end":81,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fv2.1\u002Fsamples\u002Fballoon\u002Finspect_balloon_data.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*OKE6wyZFfh2f_aZ3rd9BRw.png":{"__typename":"ImageMetadata","id":"1*OKE6wyZFfh2f_aZ3rd9BRw.png","originalHeight":583,"originalWidth":831,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_90":{"__typename":"Paragraph","id":"6d767af13c8b_90","name":"05eb","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*OKE6wyZFfh2f_aZ3rd9BRw.png"},"text":"Sample from inspect_balloon_data notebook","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_91":{"__typename":"Paragraph","id":"6d767af13c8b_91","name":"db09","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nTo create this notebook I copied inspect_data.ipynb, which we wrote for the COCO dataset, and modified one block of code at the top to load the Balloons dataset instead.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":43,"end":61,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fsamples\u002Fcoco\u002Finspect_data.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_92":{"__typename":"Paragraph","id":"6d767af13c8b_92","name":"da5a","type":"H3","href":null,"layout":null,"metadata":null,"text":"Configurations","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_93":{"__typename":"Paragraph","id":"6d767af13c8b_93","name":"e824","type":"P","href":null,"layout":null,"metadata":null,"text":"The configurations for this project are similar to the base configuration used to train the COCO dataset, so I just needed to override 3 values. As I did with the Dataset class, I inherit from the base Config class and add my overrides:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":163,"end":170,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"CODE","start":202,"end":208,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_94":{"__typename":"Paragraph","id":"6d767af13c8b_94","name":"b1b8","type":"PRE","href":null,"layout":null,"metadata":null,"text":"class BalloonConfig(Config):","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_95":{"__typename":"Paragraph","id":"6d767af13c8b_95","name":"df09","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    # Give the configuration a recognizable name\n    NAME = \"balloons\"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_96":{"__typename":"Paragraph","id":"6d767af13c8b_96","name":"569d","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    # Number of classes (including background)\n    NUM_CLASSES = 1 + 1  # Background + balloon","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_97":{"__typename":"Paragraph","id":"6d767af13c8b_97","name":"db8c","type":"PRE","href":null,"layout":null,"metadata":null,"text":"    # Number of training steps per epoch\n    STEPS_PER_EPOCH = 100","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_98":{"__typename":"Paragraph","id":"6d767af13c8b_98","name":"004b","type":"P","href":null,"layout":null,"metadata":null,"text":"The base configuration uses input images of size 1024x1024 px for best accuracy. I kept it that way. My images are a bit smaller, but the model resizes them automatically.","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_99":{"__typename":"Paragraph","id":"6d767af13c8b_99","name":"b92d","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nThe base Config class is in config.py. And BalloonConfig is in balloons.py.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":38,"end":47,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fmrcnn\u002Fconfig.py","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":72,"end":84,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fv2.1\u002Fsamples\u002Fballoon\u002Fballoon.py#L61","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_100":{"__typename":"Paragraph","id":"6d767af13c8b_100","name":"1f2b","type":"H3","href":null,"layout":null,"metadata":null,"text":"Training","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_101":{"__typename":"Paragraph","id":"6d767af13c8b_101","name":"0a7c","type":"P","href":null,"layout":null,"metadata":null,"text":"Mask R-CNN is a fairly large model. Especially that our implementation uses ResNet101 and FPN. So you need a modern GPU with 12GB of memory. It might work on less, but I haven’t tried. I used Amazon’s P2 instances to train this model, and given the small dataset, training takes less than an hour.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":192,"end":213,"href":"https:\u002F\u002Faws.amazon.com\u002Fec2\u002Finstance-types\u002Fp2\u002F","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_102":{"__typename":"Paragraph","id":"6d767af13c8b_102","name":"7837","type":"P","href":null,"layout":null,"metadata":null,"text":"Start the training with this command, running from the balloon directory. Here, we’re specifying that training should start from the pre-trained COCO weights. The code will download the weights from our repository automatically:","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"CODE","start":55,"end":62,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_103":{"__typename":"Paragraph","id":"6d767af13c8b_103","name":"c106","type":"PRE","href":null,"layout":null,"metadata":null,"text":"python3 balloon.py train --dataset=\u002Fpath\u002Fto\u002Fdataset --model=coco","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":52,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_104":{"__typename":"Paragraph","id":"6d767af13c8b_104","name":"fc26","type":"P","href":null,"layout":null,"metadata":null,"text":"And to resume training if it stopped:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_105":{"__typename":"Paragraph","id":"6d767af13c8b_105","name":"b555","type":"PRE","href":null,"layout":null,"metadata":null,"text":"python3 balloon.py train --dataset=\u002Fpath\u002Fto\u002Fdataset --model=last","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":52,"end":64,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_106":{"__typename":"Paragraph","id":"6d767af13c8b_106","name":"d71e","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nIn addition to balloons.py, the repository has three more examples: train_shapes.ipynb which trains a toy model to detect geometric shapes, coco.py which trains on the COCO dataset, and nucleus which segments nuclei in microscopy images.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":78,"end":96,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fsamples\u002Fshapes\u002Ftrain_shapes.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":150,"end":157,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fsamples\u002Fcoco\u002Fcoco.py","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":196,"end":203,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Ftree\u002Fmaster\u002Fsamples\u002Fnucleus","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":10,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_107":{"__typename":"Paragraph","id":"6d767af13c8b_107","name":"6bcb","type":"H3","href":null,"layout":null,"metadata":null,"text":"Inspecting the Results","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_108":{"__typename":"Paragraph","id":"6d767af13c8b_108","name":"a77c","type":"P","href":null,"layout":null,"metadata":null,"text":"The inspect_balloon_model notebook shows the results generated by the trained model. Check the notebook for more visualizations and a step by step walk through the detection pipeline.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":4,"end":25,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fv2.1\u002Fsamples\u002Fballoon\u002Finspect_balloon_model.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*BvqnziHW514YyO20UNtS3g.png":{"__typename":"ImageMetadata","id":"1*BvqnziHW514YyO20UNtS3g.png","originalHeight":347,"originalWidth":812,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_109":{"__typename":"Paragraph","id":"6d767af13c8b_109","name":"d499","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*BvqnziHW514YyO20UNtS3g.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_110":{"__typename":"Paragraph","id":"6d767af13c8b_110","name":"29f6","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nThis notebook is a simplified version of inspect_mode.ipynb, which includes visualizations and debugging code for the COCO dataset.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":51,"end":69,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fmaster\u002Fsamples\u002Fcoco\u002Finspect_model.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_111":{"__typename":"Paragraph","id":"6d767af13c8b_111","name":"1b36","type":"H3","href":null,"layout":null,"metadata":null,"text":"Color Splash","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_112":{"__typename":"Paragraph","id":"6d767af13c8b_112","name":"5e81","type":"P","href":null,"layout":null,"metadata":null,"text":"Finally, now that we have object masks, let’s use them to apply the color splash effect. The method is really simple: create a grayscale version of the image, and then, in areas marked by the object mask, copy back the color pixels from original image. Here is an example:","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iPAtWFnShPhX5atbY3V0pQ.png":{"__typename":"ImageMetadata","id":"1*iPAtWFnShPhX5atbY3V0pQ.png","originalHeight":494,"originalWidth":942,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_113":{"__typename":"Paragraph","id":"6d767af13c8b_113","name":"d7b5","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*iPAtWFnShPhX5atbY3V0pQ.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_114":{"__typename":"Paragraph","id":"6d767af13c8b_114","name":"5917","type":"BQ","href":null,"layout":null,"metadata":null,"text":"Code Tip:\nThe code that applies the effect is in the color_splash() function. And detect_and_color_splash() handles the whole process from loading the image, running instance segmentation, and applying the color splash filter.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":53,"end":67,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fv2.1\u002Fsamples\u002Fballoon\u002Fballoon.py#L201","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":82,"end":107,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fblob\u002Fv2.1\u002Fsamples\u002Fballoon\u002Fballoon.py#L221","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":9,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_115":{"__typename":"Paragraph","id":"6d767af13c8b_115","name":"999f","type":"H3","href":null,"layout":null,"metadata":null,"text":"FAQ","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_116":{"__typename":"Paragraph","id":"6d767af13c8b_116","name":"3c51","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Q: I want to dive deeper and understand the details, what should I read?\nA: Read these papers in this order: RCNN (pdf), Fast RCNN, Faster RCNN, FPN, Mask RCNN.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":109,"end":119,"href":"http:\u002F\u002Fciteseerx.ist.psu.edu\u002Fviewdoc\u002Fdownload;jsessionid=AF8817DD0F70B32AA08B2ECBBA8099FA?doi=10.1.1.715.2453&rep=rep1&type=pdf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":121,"end":130,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1504.08083","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":132,"end":143,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1506.01497","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":145,"end":148,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1612.03144","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"A","start":150,"end":159,"href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1703.06870","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":2,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":73,"end":75,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_117":{"__typename":"Paragraph","id":"6d767af13c8b_117","name":"7e3a","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Q: Where can I ask more questions?\nA: The Issues page on GitHub is active, you can use it for questions, as well as to report issues. Remember to search closed issues as well in case your question has been answered already.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":42,"end":63,"href":"https:\u002F\u002Fgithub.com\u002Fmatterport\u002FMask_RCNN\u002Fissues","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":2,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":35,"end":37,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_118":{"__typename":"Paragraph","id":"6d767af13c8b_118","name":"fb26","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Q: Can I contribute to this project?\nA: That would be great. Pull Requests are always welcome.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"STRONG","start":0,"end":2,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":37,"end":39,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"Paragraph:6d767af13c8b_119":{"__typename":"Paragraph","id":"6d767af13c8b_119","name":"dec0","type":"ULI","href":null,"layout":null,"metadata":null,"text":"Q: Can I join your team and work on fun projects like this one?\nA: Yes, we’re hiring for deep learning and computer vision. Apply here.","hasDropCap":null,"dropCapImage":null,"markups":[{"__typename":"Markup","type":"A","start":124,"end":134,"href":"https:\u002F\u002Fmatterport.com\u002Fcareers\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":0,"end":2,"href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","type":"STRONG","start":64,"end":66,"href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*w_ownWZZ38QhiVjVU757DA.png":{"__typename":"ImageMetadata","id":"1*w_ownWZZ38QhiVjVU757DA.png","originalHeight":621,"originalWidth":937,"focusPercentX":null,"focusPercentY":null,"alt":null},"Paragraph:6d767af13c8b_120":{"__typename":"Paragraph","id":"6d767af13c8b_120","name":"678b","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*w_ownWZZ38QhiVjVU757DA.png"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"iframe":null,"mixtapeMetadata":null},"CollectionViewerEdge:collectionId:e3a700e43a0d-viewerId:lo_8e44f571720c":{"__typename":"CollectionViewerEdge","id":"collectionId:e3a700e43a0d-viewerId:lo_8e44f571720c","isEditor":false},"User:d554379f4ffd":{"__typename":"User","id":"d554379f4ffd","atsQualifiedAt":1626295359992},"Tag:mask-rcnn":{"__typename":"Tag","id":"mask-rcnn"},"Tag:object-detection":{"__typename":"Tag","id":"object-detection"},"Tag:instance-segmentation":{"__typename":"Tag","id":"instance-segmentation"},"Tag:computer-vision":{"__typename":"Tag","id":"computer-vision"},"Tag:deep-learning":{"__typename":"Tag","id":"deep-learning"},"Collection:98111c9905da":{"__typename":"Collection","id":"98111c9905da","slug":"towards-artificial-intelligence","name":"Towards AI","domain":"pub.towardsai.net"},"User:564d903af5a2":{"__typename":"User","id":"564d903af5a2","imageId":"1*7teQ7WGLK11vff4j8_rZ1Q.png","mediumMemberAt":1654129936000,"name":"Anil Tilbe","username":"aniltilbe","customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"aniltilbe.medium.com"}},"hasSubdomain":true,"bio":"Scientist intersecting AI + Product for the U.S. Federal Government. U.S. military veteran. Harvard alumnus. Top AI, Technology, Finance Writer. See “About.”"},"ImageMetadata:1*AKTFhzlZzW5aM_VnkiD89g.jpeg":{"__typename":"ImageMetadata","id":"1*AKTFhzlZzW5aM_VnkiD89g.jpeg","alt":"A dart board","focusPercentX":null,"focusPercentY":null},"Post:60ae243e040a":{"__typename":"Post","id":"60ae243e040a","visibility":"LOCKED","previewContent":{"__typename":"PreviewContent","isFullContent":false,"subtitle":"A walkthrough of zero-shot learning"},"collection":{"__ref":"Collection:98111c9905da"},"title":"Zero-shot Learning Deep Dive: How to Select One and Present-day Challenges","mediumUrl":"https:\u002F\u002Fpub.towardsai.net\u002Fzero-shot-learning-deep-dive-how-to-select-one-and-challenges-60ae243e040a","creator":{"__ref":"User:564d903af5a2"},"previewImage":{"__ref":"ImageMetadata:1*AKTFhzlZzW5aM_VnkiD89g.jpeg"},"clapCount":53,"isSeries":false,"sequence":null,"uniqueSlug":"zero-shot-learning-deep-dive-how-to-select-one-and-challenges-60ae243e040a"},"User:53ac0949151a":{"__typename":"User","id":"53ac0949151a","imageId":"1*Y7NrYsAlCR_wcjucCYSGRw.jpeg","mediumMemberAt":0,"name":"Lucky Abolorunke","username":"luckyabolorunke","customDomainState":null,"hasSubdomain":false,"bio":"A computer science student at Fisk University; lover of Data science; I see myself as part of the shield” that guild\u002Fexplore\u002Fcommunicate the reigns\u002Fidea of data"},"ImageMetadata:1*BMnXJ6R0uukYxeN1oZnu8g.jpeg":{"__typename":"ImageMetadata","id":"1*BMnXJ6R0uukYxeN1oZnu8g.jpeg","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:d9819ca49d1c":{"__typename":"Post","id":"d9819ca49d1c","visibility":"PUBLIC","previewContent":{"__typename":"PreviewContent","isFullContent":false,"subtitle":"Building pipelines for oil and gas refinery takes more than 12 months all depending on the distance the pipes would travel, but what do…"},"collection":null,"title":"How To Build A Simple Pipeline In Machine Learning","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@luckyabolorunke\u002Fhow-to-build-a-simple-pipeline-in-machine-learning-d9819ca49d1c","creator":{"__ref":"User:53ac0949151a"},"previewImage":{"__ref":"ImageMetadata:1*BMnXJ6R0uukYxeN1oZnu8g.jpeg"},"clapCount":3,"isSeries":false,"sequence":null,"uniqueSlug":"how-to-build-a-simple-pipeline-in-machine-learning-d9819ca49d1c"},"User:2f8844ae5bf2":{"__typename":"User","id":"2f8844ae5bf2","imageId":"2*RxNpCf2tV662W5gz7xEQ8g.jpeg","mediumMemberAt":0,"name":"Saswata Chakravarty","username":"saswatac","customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"saswatac.medium.com"}},"hasSubdomain":true,"bio":"Machine Learning | Big Data | Python | Java"},"ImageMetadata:0*VInTLbxKhnOR5HD9":{"__typename":"ImageMetadata","id":"0*VInTLbxKhnOR5HD9","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:6b043212274d":{"__typename":"Post","id":"6b043212274d","visibility":"PUBLIC","previewContent":{"__typename":"PreviewContent","isFullContent":false,"subtitle":"Learn how to leverage Ray, a python library for distributed computation, to parallelise training data pipeline."},"collection":null,"title":"Speeding up data pipelines for deep learning using Ray","mediumUrl":"https:\u002F\u002Fsaswatac.medium.com\u002Fspeeding-up-data-pipelines-for-deep-learning-using-ray-6b043212274d","creator":{"__ref":"User:2f8844ae5bf2"},"previewImage":{"__ref":"ImageMetadata:0*VInTLbxKhnOR5HD9"},"clapCount":7,"isSeries":false,"sequence":null,"uniqueSlug":"speeding-up-data-pipelines-for-deep-learning-using-ray-6b043212274d"},"Collection:7f60cf5620c9":{"__typename":"Collection","id":"7f60cf5620c9","slug":"towards-data-science","name":"Towards Data Science","domain":"towardsdatascience.com"},"User:c8dcc7fb5ce5":{"__typename":"User","id":"c8dcc7fb5ce5","imageId":"2*eVvYVSkqyFwiuwJGfpCduw.png","mediumMemberAt":0,"name":"Manu Joseph","username":"manujosephv","customDomainState":null,"hasSubdomain":false,"bio":"Problem Solver, Practitioner, Researcher @ Thoucentric Analytics. https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fmanujosephv\u002F"},"ImageMetadata:0*i65JPk7qXqBIlh4t":{"__typename":"ImageMetadata","id":"0*i65JPk7qXqBIlh4t","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:c0645f794352":{"__typename":"Post","id":"c0645f794352","visibility":"LOCKED","previewContent":{"__typename":"PreviewContent","isFullContent":false,"subtitle":"Following through from my previous blog about the standard Absolute, Squared and Percent Errors, let’s take a look at the alternatives —…"},"collection":{"__ref":"Collection:7f60cf5620c9"},"title":"Forecast Error Measures: Scaled, Relative, and other Errors","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fforecast-error-measures-scaled-relative-and-other-errors-c0645f794352","creator":{"__ref":"User:c8dcc7fb5ce5"},"previewImage":{"__ref":"ImageMetadata:0*i65JPk7qXqBIlh4t"},"clapCount":50,"isSeries":false,"sequence":null,"uniqueSlug":"forecast-error-measures-scaled-relative-and-other-errors-c0645f794352"},"Collection:7219b4dc6c4c":{"__typename":"Collection","id":"7219b4dc6c4c","slug":"analytics-vidhya","name":"Analytics Vidhya","domain":null},"User:7f115b08960b":{"__typename":"User","id":"7f115b08960b","imageId":"1*efTzcp5X1xcM2Y48zhRnCQ.png","mediumMemberAt":0,"name":"Deepankar","username":"deepankar20007","customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"deepankar20007.medium.com"}},"hasSubdomain":true,"bio":"Masters student at IIITD"},"ImageMetadata:1*b53T_QLDJEekK5XnKJBSbA.png":{"__typename":"ImageMetadata","id":"1*b53T_QLDJEekK5XnKJBSbA.png","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:460436ee2393":{"__typename":"Post","id":"460436ee2393","visibility":"PUBLIC","previewContent":{"__typename":"PreviewContent","isFullContent":false,"subtitle":"Today we will discuss about recognising emotions from a person’s image. The dataset that we gonna use, is taken from kaggle and named as…"},"collection":{"__ref":"Collection:7219b4dc6c4c"},"title":"Human Face Emotion Recognition System","mediumUrl":"https:\u002F\u002Fmedium.com\u002Fanalytics-vidhya\u002Fhuman-face-emotion-recognition-system-460436ee2393","creator":{"__ref":"User:7f115b08960b"},"previewImage":{"__ref":"ImageMetadata:1*b53T_QLDJEekK5XnKJBSbA.png"},"clapCount":158,"isSeries":false,"sequence":null,"uniqueSlug":"human-face-emotion-recognition-system-460436ee2393"},"User:bb6c7e7b440f":{"__typename":"User","id":"bb6c7e7b440f","imageId":"1*jcbLQiZjnyiJwKXH6lizLw.png","mediumMemberAt":0,"name":"Pedro Carrión","username":"pedroecarrionz","customDomainState":null,"hasSubdomain":false,"bio":"I won’t better the world. I will change it."},"ImageMetadata:0*20Hl5lwM9WWpzjyX.gif":{"__typename":"ImageMetadata","id":"0*20Hl5lwM9WWpzjyX.gif","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:450160f3b43b":{"__typename":"Post","id":"450160f3b43b","visibility":"PUBLIC","previewContent":{"__typename":"PreviewContent","isFullContent":false,"subtitle":"Clustering techniques for Machine Learning."},"collection":null,"title":"Basics of Clustering","mediumUrl":"https:\u002F\u002Fmedium.com\u002F@pedroecarrionz\u002Fbasics-of-clustering-450160f3b43b","creator":{"__ref":"User:bb6c7e7b440f"},"previewImage":{"__ref":"ImageMetadata:0*20Hl5lwM9WWpzjyX.gif"},"clapCount":11,"isSeries":false,"sequence":null,"uniqueSlug":"basics-of-clustering-450160f3b43b"},"User:5e4d270d0b64":{"__typename":"User","id":"5e4d270d0b64","imageId":"2*8im3QUtZ-D-F6Fi0nGmP5w.jpeg","mediumMemberAt":0,"name":"nawaz ahmad","username":"nawazahmad20","customDomainState":null,"hasSubdomain":false,"bio":"Data science and machine learning enthusiast"},"ImageMetadata:1*opeFd5syg5Rd3M6MrHjV1g.jpeg":{"__typename":"ImageMetadata","id":"1*opeFd5syg5Rd3M6MrHjV1g.jpeg","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:4bb4f297c672":{"__typename":"Post","id":"4bb4f297c672","visibility":"LOCKED","previewContent":{"__typename":"PreviewContent","isFullContent":false,"subtitle":"Hi all, in this tutorial we’ll be learning how to make a line follower robot. Although, there are a plenty of line follower tutorials out…"},"collection":{"__ref":"Collection:7f60cf5620c9"},"title":"Line Follower Robot using CNN","mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fline-follower-robot-using-cnn-4bb4f297c672","creator":{"__ref":"User:5e4d270d0b64"},"previewImage":{"__ref":"ImageMetadata:1*opeFd5syg5Rd3M6MrHjV1g.jpeg"},"clapCount":89,"isSeries":false,"sequence":null,"uniqueSlug":"line-follower-robot-using-cnn-4bb4f297c672"},"User:153452706ad7":{"__typename":"User","id":"153452706ad7","imageId":"2*MkUxrUogzkaAyb_Nf76wRQ.jpeg","mediumMemberAt":0,"name":"Michelangiolo Mazzeschi","username":"ardito.bryan","customDomainState":null,"hasSubdomain":false,"bio":"Data Evangelist at RelevanceAI, Machine Learning expert, started my career in EY"},"ImageMetadata:0*K6IV6ZnHZ0nmq_M6":{"__typename":"ImageMetadata","id":"0*K6IV6ZnHZ0nmq_M6","alt":null,"focusPercentX":null,"focusPercentY":null},"Post:5f2e173e967e":{"__typename":"Post","id":"5f2e173e967e","visibility":"LOCKED","previewContent":{"__typename":"PreviewContent","isFullContent":false,"subtitle":"Guide to supervised learning"},"collection":{"__ref":"Collection:98111c9905da"},"title":"Wine Classifier Using Supervised Learning with 98% Accuracy","mediumUrl":"https:\u002F\u002Fpub.towardsai.net\u002Fwine-classifier-using-supervised-learning-with-98-accuracy-5f2e173e967e","creator":{"__ref":"User:153452706ad7"},"previewImage":{"__ref":"ImageMetadata:0*K6IV6ZnHZ0nmq_M6"},"clapCount":205,"isSeries":false,"sequence":null,"uniqueSlug":"wine-classifier-using-supervised-learning-with-98-accuracy-5f2e173e967e"}}</script><script src="https://cdn-client.medium.com/lite/static/js/manifest.30258cfa.js"></script><script src="https://cdn-client.medium.com/lite/static/js/221.eb6d4e84.js"></script><script src="https://cdn-client.medium.com/lite/static/js/main.e74daf95.js"></script><script src="https://cdn-client.medium.com/lite/static/js/instrumentation.d4892e93.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8732.9d4e0df2.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2837.3a7dccc7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/AppLayout.f761e915.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/reporting.bbdcaa9d.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4270.c0f5b685.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1752.a348f767.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7794.9590314e.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8316.18f2a6aa.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5221.457e73cb.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4330.1bf9fad7.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2981.c8b67800.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3115.e73260a0.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4869.15af887a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5472.5f6d4371.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9401.492bc814.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2307.2481e346.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9442.5291e270.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/7070.4ba587c4.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/4483.0a43a5ce.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/210.1b33e4a9.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/864.b1b3cbfe.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9841.1bb423da.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1681.08ba3d39.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1018.7751940c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9304.0cb94a81.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/1579.d42a41fe.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/8051.c536c001.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/334.1987698a.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/251.80d522dd.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/5754.6687b8d5.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostPage.MainContent.9170140c.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/3702.99c8e9df.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/2021.fa3d5176.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/9291.54612f18.chunk.js"></script>
<script src="https://cdn-client.medium.com/lite/static/js/PostPage.RightColumnContent.1629a6fb.chunk.js"></script><script>window.main();</script></body></html>